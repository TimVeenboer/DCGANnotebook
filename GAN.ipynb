{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYfh_dop2jwH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU-ybKXEQwJM"
      },
      "source": [
        "# Setting the constants\n",
        "BATCHSIZE, CHANNELS, WIDTH, HEIGHT = 100, 1, 28, 28\n",
        "FAKE, REAL = 0, 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSWSU-db2rgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0d4a4a-60e2-4e0a-8db5-72c6a6285035"
      },
      "source": [
        "### Downloading the MNIST dataset\n",
        "\n",
        "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                             ])\n",
        "mnist_set = MNIST(root='./', download=True, transform=transform)\n",
        "dataset = DataLoader(mnist_set, batch_size=BATCHSIZE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-30 11:08:04--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-04-30 11:08:04--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz’\n",
            "\n",
            "MNIST.tar.gz            [      <=>           ]  33.20M  31.1MB/s    in 1.1s    \n",
            "\n",
            "2021-04-30 11:08:05 (31.1 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hIDcRtx5A9_"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  \"\"\"Architecture of the discriminator that distinguishes real and generated\n",
        "     images from each other.\"\"\"\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(CHANNELS, 32, kernel_size=7, stride=1)\n",
        "    self.batchnorm1 = nn.BatchNorm2d(32)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=8, stride=1)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "    self.conv3 = nn.Conv2d(64, 1, kernel_size=8, stride=8)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    output = self.conv1(input)\n",
        "    output = F.leaky_relu(output)\n",
        "    output = self.batchnorm1(output)\n",
        "    output = self.conv2(output)\n",
        "    output = F.leaky_relu(output)\n",
        "    output = self.batchnorm2(output)\n",
        "    output = self.conv3(output)\n",
        "    output = torch.sigmoid(output)\n",
        "    return output"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWPXpfldICp3"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"Architecture for the generator. It generates an image based on noise\n",
        "     created from a zero-mean gaussian.\"\"\"\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.conv1 = nn.ConvTranspose2d(BATCHSIZE, 512, 4, 1, 0)\n",
        "    self.batchnorm1 = nn.BatchNorm2d(512)\n",
        "    self.conv2 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(256)\n",
        "    self.conv3 = nn.ConvTranspose2d(256, 128, 4, 2, 2)\n",
        "    self.batchnorm3 = nn.BatchNorm2d(128)\n",
        "    self.conv4 = nn.ConvTranspose2d(128, 64, 4, 1, 1)\n",
        "    self.batchnorm4 = nn.BatchNorm2d(64)\n",
        "    self.conv5 = nn.ConvTranspose2d(64, 1, 4, 2, 2)\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.batchnorm1(F.relu(self.conv1(input)))\n",
        "    output = self.batchnorm2(F.relu(self.conv2(output)))\n",
        "    output = self.batchnorm3(F.relu(self.conv3(output)))\n",
        "    output = self.batchnorm4(F.relu(self.conv4(output)))\n",
        "    output = torch.tanh(self.conv5(output))\n",
        "    return output"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxfQwDAxRI_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867832a4-20b3-4de2-e314-394e8c80abeb"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "D = Discriminator().to(device)\n",
        "G = Generator().to(device)\n",
        "# loss function, binary cross-entropy since we have 0 and 1 as labels\n",
        "bce = nn.BCELoss()\n",
        "optimizerD = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
        "optimizerG = torch.optim.Adam(G.parameters(), lr=0.0002)\n",
        "epochs = 7\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i, data in enumerate(dataset):\n",
        "    D.zero_grad()\n",
        "    # train the discriminator on real numbers \n",
        "    # create labels for real images\n",
        "    label = torch.full((BATCHSIZE,), REAL, dtype=torch.float, device=device)\n",
        "    # make predictions with the discriminator\n",
        "    predictions = D(data[0].cuda()).to(device).view(-1)\n",
        "    # get and update the gradients with the real labels\n",
        "    realError = bce(predictions, label)\n",
        "    realError.backward()\n",
        "\n",
        "    # train the discriminator on fake generated images\n",
        "    # create the fake labels \n",
        "    label.fill_(FAKE)\n",
        "    # create noise\n",
        "    noise = torch.randn(BATCHSIZE, BATCHSIZE, 1, 1, device=device)\n",
        "    # create a fake image from the noise\n",
        "    fake_images = G(noise).to(device)\n",
        "    # make predictions with the discriminator\n",
        "    predictions = D(fake_images.detach()).to(device).view(-1)\n",
        "    # get and update the gradients with the fake labels\n",
        "    fakeError = bce(predictions, label)\n",
        "    fakeError.backward()\n",
        "    addedError = realError + fakeError\n",
        "    optimizerD.step()\n",
        "\n",
        "    # train the generator \n",
        "    G.zero_grad()\n",
        "    # initialize real labels again\n",
        "    label.fill_(REAL)\n",
        "    # make predictions with the discriminator for the fake images\n",
        "    predictions = D(fake_images).to(device).view(-1)\n",
        "    # train the generator with these predictions and the real labels\n",
        "    generatorError = bce(predictions, label)\n",
        "    generatorError.backward()\n",
        "    optimizerG.step()\n",
        "    print('Samples: {}/60000\\nEpoch: {}\\nLoss D: {}\\nLoss G: {}\\n'.format((i+1)*BATCHSIZE, epoch, addedError, generatorError))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Samples: 20100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5564233660697937\n",
            "Loss G: 2.568850040435791\n",
            "\n",
            "Samples: 20200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6426974534988403\n",
            "Loss G: 2.583766460418701\n",
            "\n",
            "Samples: 20300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6934854984283447\n",
            "Loss G: 2.385523557662964\n",
            "\n",
            "Samples: 20400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4812513291835785\n",
            "Loss G: 2.6623566150665283\n",
            "\n",
            "Samples: 20500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5878605842590332\n",
            "Loss G: 2.2090606689453125\n",
            "\n",
            "Samples: 20600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6595326066017151\n",
            "Loss G: 2.1740901470184326\n",
            "\n",
            "Samples: 20700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.39506274461746216\n",
            "Loss G: 2.123553514480591\n",
            "\n",
            "Samples: 20800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.3898337483406067\n",
            "Loss G: 2.151378631591797\n",
            "\n",
            "Samples: 20900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5685178637504578\n",
            "Loss G: 2.1316280364990234\n",
            "\n",
            "Samples: 21000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.585817813873291\n",
            "Loss G: 2.224043846130371\n",
            "\n",
            "Samples: 21100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5002283453941345\n",
            "Loss G: 2.1927435398101807\n",
            "\n",
            "Samples: 21200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5105293989181519\n",
            "Loss G: 2.4474916458129883\n",
            "\n",
            "Samples: 21300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.42540818452835083\n",
            "Loss G: 2.720266580581665\n",
            "\n",
            "Samples: 21400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6615920662879944\n",
            "Loss G: 2.6280059814453125\n",
            "\n",
            "Samples: 21500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4934583604335785\n",
            "Loss G: 2.773388624191284\n",
            "\n",
            "Samples: 21600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.48549163341522217\n",
            "Loss G: 2.807180166244507\n",
            "\n",
            "Samples: 21700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.37307193875312805\n",
            "Loss G: 2.708817720413208\n",
            "\n",
            "Samples: 21800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4198343753814697\n",
            "Loss G: 2.876723527908325\n",
            "\n",
            "Samples: 21900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6122804880142212\n",
            "Loss G: 2.697706937789917\n",
            "\n",
            "Samples: 22000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.610788106918335\n",
            "Loss G: 2.625283718109131\n",
            "\n",
            "Samples: 22100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7012878656387329\n",
            "Loss G: 2.545969009399414\n",
            "\n",
            "Samples: 22200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7967990636825562\n",
            "Loss G: 2.555882692337036\n",
            "\n",
            "Samples: 22300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.542172908782959\n",
            "Loss G: 2.3145456314086914\n",
            "\n",
            "Samples: 22400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6837992668151855\n",
            "Loss G: 2.1429224014282227\n",
            "\n",
            "Samples: 22500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.690743088722229\n",
            "Loss G: 2.243232488632202\n",
            "\n",
            "Samples: 22600/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.061624526977539\n",
            "Loss G: 2.1185991764068604\n",
            "\n",
            "Samples: 22700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8291196227073669\n",
            "Loss G: 1.9700006246566772\n",
            "\n",
            "Samples: 22800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8142977952957153\n",
            "Loss G: 1.6942282915115356\n",
            "\n",
            "Samples: 22900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8759600520133972\n",
            "Loss G: 1.7853106260299683\n",
            "\n",
            "Samples: 23000/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0180028676986694\n",
            "Loss G: 1.685850739479065\n",
            "\n",
            "Samples: 23100/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0239944458007812\n",
            "Loss G: 1.9885175228118896\n",
            "\n",
            "Samples: 23200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9090480804443359\n",
            "Loss G: 2.253363609313965\n",
            "\n",
            "Samples: 23300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6178842782974243\n",
            "Loss G: 2.737116575241089\n",
            "\n",
            "Samples: 23400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.654690146446228\n",
            "Loss G: 2.83227276802063\n",
            "\n",
            "Samples: 23500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7154024839401245\n",
            "Loss G: 2.7131590843200684\n",
            "\n",
            "Samples: 23600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7832245230674744\n",
            "Loss G: 2.3836312294006348\n",
            "\n",
            "Samples: 23700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8615440726280212\n",
            "Loss G: 2.2465755939483643\n",
            "\n",
            "Samples: 23800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7915081977844238\n",
            "Loss G: 1.8462945222854614\n",
            "\n",
            "Samples: 23900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8485251069068909\n",
            "Loss G: 1.6858822107315063\n",
            "\n",
            "Samples: 24000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6260956525802612\n",
            "Loss G: 1.9489144086837769\n",
            "\n",
            "Samples: 24100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5567457675933838\n",
            "Loss G: 2.1513192653656006\n",
            "\n",
            "Samples: 24200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5177579522132874\n",
            "Loss G: 2.3375682830810547\n",
            "\n",
            "Samples: 24300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8056305646896362\n",
            "Loss G: 2.443011999130249\n",
            "\n",
            "Samples: 24400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5307486653327942\n",
            "Loss G: 2.618665933609009\n",
            "\n",
            "Samples: 24500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8873045444488525\n",
            "Loss G: 2.1367573738098145\n",
            "\n",
            "Samples: 24600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9787102937698364\n",
            "Loss G: 1.8252302408218384\n",
            "\n",
            "Samples: 24700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8287308216094971\n",
            "Loss G: 1.5045626163482666\n",
            "\n",
            "Samples: 24800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7703893184661865\n",
            "Loss G: 1.5705257654190063\n",
            "\n",
            "Samples: 24900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7726665735244751\n",
            "Loss G: 1.6211985349655151\n",
            "\n",
            "Samples: 25000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7713849544525146\n",
            "Loss G: 1.954073429107666\n",
            "\n",
            "Samples: 25100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7739976644515991\n",
            "Loss G: 1.928234338760376\n",
            "\n",
            "Samples: 25200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7354562282562256\n",
            "Loss G: 2.198582410812378\n",
            "\n",
            "Samples: 25300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7568528652191162\n",
            "Loss G: 2.346256971359253\n",
            "\n",
            "Samples: 25400/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.1269702911376953\n",
            "Loss G: 2.042283058166504\n",
            "\n",
            "Samples: 25500/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0214002132415771\n",
            "Loss G: 1.4872558116912842\n",
            "\n",
            "Samples: 25600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8894779682159424\n",
            "Loss G: 1.3328698873519897\n",
            "\n",
            "Samples: 25700/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.092755675315857\n",
            "Loss G: 1.4555824995040894\n",
            "\n",
            "Samples: 25800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9627364873886108\n",
            "Loss G: 1.551410436630249\n",
            "\n",
            "Samples: 25900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8440465331077576\n",
            "Loss G: 1.6865899562835693\n",
            "\n",
            "Samples: 26000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8855617046356201\n",
            "Loss G: 1.9902095794677734\n",
            "\n",
            "Samples: 26100/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0779002904891968\n",
            "Loss G: 1.8302819728851318\n",
            "\n",
            "Samples: 26200/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0866690874099731\n",
            "Loss G: 1.815631628036499\n",
            "\n",
            "Samples: 26300/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.122745394706726\n",
            "Loss G: 1.4078530073165894\n",
            "\n",
            "Samples: 26400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8551439046859741\n",
            "Loss G: 1.2711225748062134\n",
            "\n",
            "Samples: 26500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9011732935905457\n",
            "Loss G: 1.280476450920105\n",
            "\n",
            "Samples: 26600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9246069192886353\n",
            "Loss G: 1.4090908765792847\n",
            "\n",
            "Samples: 26700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9974044561386108\n",
            "Loss G: 1.8157671689987183\n",
            "\n",
            "Samples: 26800/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.2400147914886475\n",
            "Loss G: 1.7787466049194336\n",
            "\n",
            "Samples: 26900/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0546938180923462\n",
            "Loss G: 1.6080633401870728\n",
            "\n",
            "Samples: 27000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6706889867782593\n",
            "Loss G: 1.6347399950027466\n",
            "\n",
            "Samples: 27100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9105937480926514\n",
            "Loss G: 1.4574644565582275\n",
            "\n",
            "Samples: 27200/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.1931630373001099\n",
            "Loss G: 1.5407531261444092\n",
            "\n",
            "Samples: 27300/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.1175320148468018\n",
            "Loss G: 1.322381854057312\n",
            "\n",
            "Samples: 27400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.81394362449646\n",
            "Loss G: 1.65483820438385\n",
            "\n",
            "Samples: 27500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8651507496833801\n",
            "Loss G: 1.8115520477294922\n",
            "\n",
            "Samples: 27600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6227742433547974\n",
            "Loss G: 2.037097930908203\n",
            "\n",
            "Samples: 27700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7609955072402954\n",
            "Loss G: 2.0328595638275146\n",
            "\n",
            "Samples: 27800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6717622876167297\n",
            "Loss G: 2.314279794692993\n",
            "\n",
            "Samples: 27900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7030165791511536\n",
            "Loss G: 2.004215717315674\n",
            "\n",
            "Samples: 28000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9677186012268066\n",
            "Loss G: 1.9153093099594116\n",
            "\n",
            "Samples: 28100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.806286096572876\n",
            "Loss G: 1.4291834831237793\n",
            "\n",
            "Samples: 28200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7773502469062805\n",
            "Loss G: 1.3510998487472534\n",
            "\n",
            "Samples: 28300/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0131045579910278\n",
            "Loss G: 1.264277458190918\n",
            "\n",
            "Samples: 28400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.826567530632019\n",
            "Loss G: 1.6939144134521484\n",
            "\n",
            "Samples: 28500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7865970134735107\n",
            "Loss G: 1.9259761571884155\n",
            "\n",
            "Samples: 28600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6323540210723877\n",
            "Loss G: 2.4748849868774414\n",
            "\n",
            "Samples: 28700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7327138781547546\n",
            "Loss G: 2.5534298419952393\n",
            "\n",
            "Samples: 28800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9276851415634155\n",
            "Loss G: 2.502683401107788\n",
            "\n",
            "Samples: 28900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.620471715927124\n",
            "Loss G: 2.355113983154297\n",
            "\n",
            "Samples: 29000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6484020352363586\n",
            "Loss G: 2.046926736831665\n",
            "\n",
            "Samples: 29100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6980773210525513\n",
            "Loss G: 1.7778799533843994\n",
            "\n",
            "Samples: 29200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5419341921806335\n",
            "Loss G: 2.0939090251922607\n",
            "\n",
            "Samples: 29300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6426330208778381\n",
            "Loss G: 1.5792429447174072\n",
            "\n",
            "Samples: 29400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8375344276428223\n",
            "Loss G: 1.8179430961608887\n",
            "\n",
            "Samples: 29500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6856738328933716\n",
            "Loss G: 1.9395556449890137\n",
            "\n",
            "Samples: 29600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5998592376708984\n",
            "Loss G: 2.2062501907348633\n",
            "\n",
            "Samples: 29700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6970683336257935\n",
            "Loss G: 2.10988187789917\n",
            "\n",
            "Samples: 29800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7416459321975708\n",
            "Loss G: 2.4382753372192383\n",
            "\n",
            "Samples: 29900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8672894239425659\n",
            "Loss G: 2.1003053188323975\n",
            "\n",
            "Samples: 30000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5408679246902466\n",
            "Loss G: 2.3628900051116943\n",
            "\n",
            "Samples: 30100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6383868455886841\n",
            "Loss G: 1.8007311820983887\n",
            "\n",
            "Samples: 30200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6928983926773071\n",
            "Loss G: 1.7602068185806274\n",
            "\n",
            "Samples: 30300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7024561166763306\n",
            "Loss G: 1.9424141645431519\n",
            "\n",
            "Samples: 30400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.940805196762085\n",
            "Loss G: 2.2988486289978027\n",
            "\n",
            "Samples: 30500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7474621534347534\n",
            "Loss G: 2.279702663421631\n",
            "\n",
            "Samples: 30600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.75886070728302\n",
            "Loss G: 2.2685916423797607\n",
            "\n",
            "Samples: 30700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.958014726638794\n",
            "Loss G: 1.8874398469924927\n",
            "\n",
            "Samples: 30800/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0158847570419312\n",
            "Loss G: 1.5709538459777832\n",
            "\n",
            "Samples: 30900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8857522010803223\n",
            "Loss G: 1.3493036031723022\n",
            "\n",
            "Samples: 31000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8265334367752075\n",
            "Loss G: 1.6004142761230469\n",
            "\n",
            "Samples: 31100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9065556526184082\n",
            "Loss G: 1.5278257131576538\n",
            "\n",
            "Samples: 31200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.761279821395874\n",
            "Loss G: 2.1882007122039795\n",
            "\n",
            "Samples: 31300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9365684986114502\n",
            "Loss G: 2.250366687774658\n",
            "\n",
            "Samples: 31400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9141131043434143\n",
            "Loss G: 2.3155698776245117\n",
            "\n",
            "Samples: 31500/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0466883182525635\n",
            "Loss G: 2.0156984329223633\n",
            "\n",
            "Samples: 31600/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.1364130973815918\n",
            "Loss G: 1.4801360368728638\n",
            "\n",
            "Samples: 31700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9494985938072205\n",
            "Loss G: 1.2637587785720825\n",
            "\n",
            "Samples: 31800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.928308367729187\n",
            "Loss G: 1.3211610317230225\n",
            "\n",
            "Samples: 31900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.87907475233078\n",
            "Loss G: 1.64393150806427\n",
            "\n",
            "Samples: 32000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7627400755882263\n",
            "Loss G: 1.996675968170166\n",
            "\n",
            "Samples: 32100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7671375870704651\n",
            "Loss G: 1.8141692876815796\n",
            "\n",
            "Samples: 32200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7048700451850891\n",
            "Loss G: 1.9464153051376343\n",
            "\n",
            "Samples: 32300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.633522093296051\n",
            "Loss G: 2.1868896484375\n",
            "\n",
            "Samples: 32400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7723628878593445\n",
            "Loss G: 1.80204439163208\n",
            "\n",
            "Samples: 32500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7414634227752686\n",
            "Loss G: 1.4297394752502441\n",
            "\n",
            "Samples: 32600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5022489428520203\n",
            "Loss G: 1.6982635259628296\n",
            "\n",
            "Samples: 32700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6428728103637695\n",
            "Loss G: 1.7249398231506348\n",
            "\n",
            "Samples: 32800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5062499046325684\n",
            "Loss G: 2.312856435775757\n",
            "\n",
            "Samples: 32900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5294222831726074\n",
            "Loss G: 2.3714940547943115\n",
            "\n",
            "Samples: 33000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6434556245803833\n",
            "Loss G: 2.5335776805877686\n",
            "\n",
            "Samples: 33100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6441635489463806\n",
            "Loss G: 2.691880941390991\n",
            "\n",
            "Samples: 33200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7213912606239319\n",
            "Loss G: 2.3433187007904053\n",
            "\n",
            "Samples: 33300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4430919885635376\n",
            "Loss G: 2.014122724533081\n",
            "\n",
            "Samples: 33400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6437790393829346\n",
            "Loss G: 1.6929988861083984\n",
            "\n",
            "Samples: 33500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4710785150527954\n",
            "Loss G: 2.104273557662964\n",
            "\n",
            "Samples: 33600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.375738263130188\n",
            "Loss G: 2.3846230506896973\n",
            "\n",
            "Samples: 33700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4197043180465698\n",
            "Loss G: 2.4532382488250732\n",
            "\n",
            "Samples: 33800/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0057141780853271\n",
            "Loss G: 2.211725950241089\n",
            "\n",
            "Samples: 33900/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0450973510742188\n",
            "Loss G: 1.6286922693252563\n",
            "\n",
            "Samples: 34000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5953065156936646\n",
            "Loss G: 1.4074459075927734\n",
            "\n",
            "Samples: 34100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7741214036941528\n",
            "Loss G: 1.2770766019821167\n",
            "\n",
            "Samples: 34200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7002345323562622\n",
            "Loss G: 1.6242531538009644\n",
            "\n",
            "Samples: 34300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6604917049407959\n",
            "Loss G: 2.0784971714019775\n",
            "\n",
            "Samples: 34400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5650399923324585\n",
            "Loss G: 2.7517547607421875\n",
            "\n",
            "Samples: 34500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5522404909133911\n",
            "Loss G: 2.73183536529541\n",
            "\n",
            "Samples: 34600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5928407907485962\n",
            "Loss G: 2.6609103679656982\n",
            "\n",
            "Samples: 34700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9521172642707825\n",
            "Loss G: 2.582688570022583\n",
            "\n",
            "Samples: 34800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6638412475585938\n",
            "Loss G: 1.6870522499084473\n",
            "\n",
            "Samples: 34900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6264225840568542\n",
            "Loss G: 1.3702468872070312\n",
            "\n",
            "Samples: 35000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.830823540687561\n",
            "Loss G: 1.051594853401184\n",
            "\n",
            "Samples: 35100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6763662099838257\n",
            "Loss G: 1.377060055732727\n",
            "\n",
            "Samples: 35200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6376619338989258\n",
            "Loss G: 1.7865737676620483\n",
            "\n",
            "Samples: 35300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6231566071510315\n",
            "Loss G: 2.1178953647613525\n",
            "\n",
            "Samples: 35400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7643109560012817\n",
            "Loss G: 2.4127228260040283\n",
            "\n",
            "Samples: 35500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6086946725845337\n",
            "Loss G: 2.8399107456207275\n",
            "\n",
            "Samples: 35600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.75244140625\n",
            "Loss G: 2.389681577682495\n",
            "\n",
            "Samples: 35700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.49510109424591064\n",
            "Loss G: 2.0522940158843994\n",
            "\n",
            "Samples: 35800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6738541126251221\n",
            "Loss G: 1.865427017211914\n",
            "\n",
            "Samples: 35900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5252137184143066\n",
            "Loss G: 1.6436001062393188\n",
            "\n",
            "Samples: 36000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.48138394951820374\n",
            "Loss G: 1.9544800519943237\n",
            "\n",
            "Samples: 36100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.505616307258606\n",
            "Loss G: 2.0044870376586914\n",
            "\n",
            "Samples: 36200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.60419100522995\n",
            "Loss G: 2.0265398025512695\n",
            "\n",
            "Samples: 36300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7206752896308899\n",
            "Loss G: 2.033616781234741\n",
            "\n",
            "Samples: 36400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5436826348304749\n",
            "Loss G: 1.8448971509933472\n",
            "\n",
            "Samples: 36500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4816030263900757\n",
            "Loss G: 1.789617896080017\n",
            "\n",
            "Samples: 36600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6773289442062378\n",
            "Loss G: 1.845947504043579\n",
            "\n",
            "Samples: 36700/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.1703057289123535\n",
            "Loss G: 1.7641921043395996\n",
            "\n",
            "Samples: 36800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8800075054168701\n",
            "Loss G: 1.269301176071167\n",
            "\n",
            "Samples: 36900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6462644338607788\n",
            "Loss G: 1.4115873575210571\n",
            "\n",
            "Samples: 37000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7698556780815125\n",
            "Loss G: 1.3029190301895142\n",
            "\n",
            "Samples: 37100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7545710802078247\n",
            "Loss G: 1.5766263008117676\n",
            "\n",
            "Samples: 37200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6943035125732422\n",
            "Loss G: 1.8248748779296875\n",
            "\n",
            "Samples: 37300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7167021036148071\n",
            "Loss G: 1.787414789199829\n",
            "\n",
            "Samples: 37400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6383216381072998\n",
            "Loss G: 1.8130837678909302\n",
            "\n",
            "Samples: 37500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.45644986629486084\n",
            "Loss G: 2.0600106716156006\n",
            "\n",
            "Samples: 37600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5419028997421265\n",
            "Loss G: 2.160719394683838\n",
            "\n",
            "Samples: 37700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4962727725505829\n",
            "Loss G: 2.3617494106292725\n",
            "\n",
            "Samples: 37800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5340920090675354\n",
            "Loss G: 2.363008737564087\n",
            "\n",
            "Samples: 37900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4989771842956543\n",
            "Loss G: 2.3821539878845215\n",
            "\n",
            "Samples: 38000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5469470620155334\n",
            "Loss G: 2.5270047187805176\n",
            "\n",
            "Samples: 38100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.519897997379303\n",
            "Loss G: 2.2692935466766357\n",
            "\n",
            "Samples: 38200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.3352126479148865\n",
            "Loss G: 2.397632122039795\n",
            "\n",
            "Samples: 38300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4158936142921448\n",
            "Loss G: 2.2148725986480713\n",
            "\n",
            "Samples: 38400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.48021119832992554\n",
            "Loss G: 2.072497844696045\n",
            "\n",
            "Samples: 38500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5101462602615356\n",
            "Loss G: 2.106715679168701\n",
            "\n",
            "Samples: 38600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.610727846622467\n",
            "Loss G: 1.9756507873535156\n",
            "\n",
            "Samples: 38700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6993769407272339\n",
            "Loss G: 2.2296056747436523\n",
            "\n",
            "Samples: 38800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7407557964324951\n",
            "Loss G: 2.2373135089874268\n",
            "\n",
            "Samples: 38900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5475757718086243\n",
            "Loss G: 2.36250638961792\n",
            "\n",
            "Samples: 39000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5966464281082153\n",
            "Loss G: 2.326820135116577\n",
            "\n",
            "Samples: 39100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6915402412414551\n",
            "Loss G: 2.3448071479797363\n",
            "\n",
            "Samples: 39200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8407025933265686\n",
            "Loss G: 2.080981969833374\n",
            "\n",
            "Samples: 39300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6854507923126221\n",
            "Loss G: 2.2970693111419678\n",
            "\n",
            "Samples: 39400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8121660351753235\n",
            "Loss G: 1.911683440208435\n",
            "\n",
            "Samples: 39500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6142802238464355\n",
            "Loss G: 2.3893356323242188\n",
            "\n",
            "Samples: 39600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7736082077026367\n",
            "Loss G: 2.022549629211426\n",
            "\n",
            "Samples: 39700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.740868091583252\n",
            "Loss G: 2.0478391647338867\n",
            "\n",
            "Samples: 39800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8185648918151855\n",
            "Loss G: 2.1143624782562256\n",
            "\n",
            "Samples: 39900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8937803506851196\n",
            "Loss G: 2.482448101043701\n",
            "\n",
            "Samples: 40000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8449825048446655\n",
            "Loss G: 2.328162670135498\n",
            "\n",
            "Samples: 40100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6063718795776367\n",
            "Loss G: 2.369434356689453\n",
            "\n",
            "Samples: 40200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7417725324630737\n",
            "Loss G: 2.4500997066497803\n",
            "\n",
            "Samples: 40300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6099656224250793\n",
            "Loss G: 2.6130950450897217\n",
            "\n",
            "Samples: 40400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5236115455627441\n",
            "Loss G: 2.3575901985168457\n",
            "\n",
            "Samples: 40500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5811737775802612\n",
            "Loss G: 2.1803665161132812\n",
            "\n",
            "Samples: 40600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5406098365783691\n",
            "Loss G: 2.577826976776123\n",
            "\n",
            "Samples: 40700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6344677209854126\n",
            "Loss G: 2.2532706260681152\n",
            "\n",
            "Samples: 40800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8351969718933105\n",
            "Loss G: 2.4076755046844482\n",
            "\n",
            "Samples: 40900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8230040073394775\n",
            "Loss G: 2.491427183151245\n",
            "\n",
            "Samples: 41000/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.122365951538086\n",
            "Loss G: 1.9819551706314087\n",
            "\n",
            "Samples: 41100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7955306768417358\n",
            "Loss G: 1.928667664527893\n",
            "\n",
            "Samples: 41200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7232993841171265\n",
            "Loss G: 1.613617181777954\n",
            "\n",
            "Samples: 41300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6645941138267517\n",
            "Loss G: 1.8660436868667603\n",
            "\n",
            "Samples: 41400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5572217702865601\n",
            "Loss G: 2.1680939197540283\n",
            "\n",
            "Samples: 41500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6178804636001587\n",
            "Loss G: 2.193804979324341\n",
            "\n",
            "Samples: 41600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5769535303115845\n",
            "Loss G: 2.371840715408325\n",
            "\n",
            "Samples: 41700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6454312801361084\n",
            "Loss G: 2.4247169494628906\n",
            "\n",
            "Samples: 41800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9600610733032227\n",
            "Loss G: 2.151484727859497\n",
            "\n",
            "Samples: 41900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.620490550994873\n",
            "Loss G: 2.2330050468444824\n",
            "\n",
            "Samples: 42000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6148244738578796\n",
            "Loss G: 1.9960435628890991\n",
            "\n",
            "Samples: 42100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5895631313323975\n",
            "Loss G: 1.898232340812683\n",
            "\n",
            "Samples: 42200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6092807650566101\n",
            "Loss G: 1.9322363138198853\n",
            "\n",
            "Samples: 42300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6894208788871765\n",
            "Loss G: 1.936279535293579\n",
            "\n",
            "Samples: 42400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9459964036941528\n",
            "Loss G: 1.9953960180282593\n",
            "\n",
            "Samples: 42500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9601186513900757\n",
            "Loss G: 2.270782947540283\n",
            "\n",
            "Samples: 42600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7816852331161499\n",
            "Loss G: 2.118842363357544\n",
            "\n",
            "Samples: 42700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7282328605651855\n",
            "Loss G: 1.8035297393798828\n",
            "\n",
            "Samples: 42800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6131625175476074\n",
            "Loss G: 1.8920300006866455\n",
            "\n",
            "Samples: 42900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7340298891067505\n",
            "Loss G: 1.8531001806259155\n",
            "\n",
            "Samples: 43000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7579033374786377\n",
            "Loss G: 1.922579288482666\n",
            "\n",
            "Samples: 43100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.755864143371582\n",
            "Loss G: 1.7285455465316772\n",
            "\n",
            "Samples: 43200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7389969825744629\n",
            "Loss G: 1.7997692823410034\n",
            "\n",
            "Samples: 43300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6881283521652222\n",
            "Loss G: 2.0054988861083984\n",
            "\n",
            "Samples: 43400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7194069027900696\n",
            "Loss G: 2.166001319885254\n",
            "\n",
            "Samples: 43500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9940090179443359\n",
            "Loss G: 2.170710802078247\n",
            "\n",
            "Samples: 43600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7817715406417847\n",
            "Loss G: 2.083522081375122\n",
            "\n",
            "Samples: 43700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7927020788192749\n",
            "Loss G: 1.7261213064193726\n",
            "\n",
            "Samples: 43800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9097636938095093\n",
            "Loss G: 1.5766228437423706\n",
            "\n",
            "Samples: 43900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8517215251922607\n",
            "Loss G: 1.7664449214935303\n",
            "\n",
            "Samples: 44000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8115184307098389\n",
            "Loss G: 1.9111707210540771\n",
            "\n",
            "Samples: 44100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5843826532363892\n",
            "Loss G: 2.3053994178771973\n",
            "\n",
            "Samples: 44200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7841840982437134\n",
            "Loss G: 2.2506048679351807\n",
            "\n",
            "Samples: 44300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9072819948196411\n",
            "Loss G: 1.8944945335388184\n",
            "\n",
            "Samples: 44400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7415737509727478\n",
            "Loss G: 1.9557818174362183\n",
            "\n",
            "Samples: 44500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6333190202713013\n",
            "Loss G: 2.0329439640045166\n",
            "\n",
            "Samples: 44600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6567121744155884\n",
            "Loss G: 1.7897826433181763\n",
            "\n",
            "Samples: 44700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6581128835678101\n",
            "Loss G: 2.117605447769165\n",
            "\n",
            "Samples: 44800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5927925109863281\n",
            "Loss G: 2.2781007289886475\n",
            "\n",
            "Samples: 44900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5044471621513367\n",
            "Loss G: 2.3649861812591553\n",
            "\n",
            "Samples: 45000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.741128146648407\n",
            "Loss G: 2.631699800491333\n",
            "\n",
            "Samples: 45100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6640973091125488\n",
            "Loss G: 2.160449743270874\n",
            "\n",
            "Samples: 45200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5728810429573059\n",
            "Loss G: 2.268665313720703\n",
            "\n",
            "Samples: 45300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5688730478286743\n",
            "Loss G: 2.1014626026153564\n",
            "\n",
            "Samples: 45400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5072248578071594\n",
            "Loss G: 2.3063724040985107\n",
            "\n",
            "Samples: 45500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5359592437744141\n",
            "Loss G: 2.094370126724243\n",
            "\n",
            "Samples: 45600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5599308013916016\n",
            "Loss G: 2.1104788780212402\n",
            "\n",
            "Samples: 45700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5341607928276062\n",
            "Loss G: 2.207547187805176\n",
            "\n",
            "Samples: 45800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.594760537147522\n",
            "Loss G: 2.3269829750061035\n",
            "\n",
            "Samples: 45900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4200417101383209\n",
            "Loss G: 2.317115545272827\n",
            "\n",
            "Samples: 46000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7251412868499756\n",
            "Loss G: 1.9882992506027222\n",
            "\n",
            "Samples: 46100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7063146233558655\n",
            "Loss G: 1.9013020992279053\n",
            "\n",
            "Samples: 46200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6954495310783386\n",
            "Loss G: 1.886110782623291\n",
            "\n",
            "Samples: 46300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5677741169929504\n",
            "Loss G: 2.2433133125305176\n",
            "\n",
            "Samples: 46400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.611376166343689\n",
            "Loss G: 1.9929286241531372\n",
            "\n",
            "Samples: 46500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.744840145111084\n",
            "Loss G: 1.94245445728302\n",
            "\n",
            "Samples: 46600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6238247156143188\n",
            "Loss G: 2.255315065383911\n",
            "\n",
            "Samples: 46700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6273354887962341\n",
            "Loss G: 2.364009141921997\n",
            "\n",
            "Samples: 46800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6405860781669617\n",
            "Loss G: 2.143778085708618\n",
            "\n",
            "Samples: 46900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5208315253257751\n",
            "Loss G: 2.049431324005127\n",
            "\n",
            "Samples: 47000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7679031491279602\n",
            "Loss G: 2.078352928161621\n",
            "\n",
            "Samples: 47100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8374511003494263\n",
            "Loss G: 1.3915197849273682\n",
            "\n",
            "Samples: 47200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8374431133270264\n",
            "Loss G: 1.2478501796722412\n",
            "\n",
            "Samples: 47300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7570615410804749\n",
            "Loss G: 1.97005033493042\n",
            "\n",
            "Samples: 47400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7237236499786377\n",
            "Loss G: 2.2994515895843506\n",
            "\n",
            "Samples: 47500/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.6255909204483032\n",
            "Loss G: 2.3727684020996094\n",
            "\n",
            "Samples: 47600/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.4419243335723877\n",
            "Loss G: 1.264717936515808\n",
            "\n",
            "Samples: 47700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9926083087921143\n",
            "Loss G: 1.042716383934021\n",
            "\n",
            "Samples: 47800/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0180745124816895\n",
            "Loss G: 1.1583118438720703\n",
            "\n",
            "Samples: 47900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9611313939094543\n",
            "Loss G: 1.5636779069900513\n",
            "\n",
            "Samples: 48000/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0989596843719482\n",
            "Loss G: 1.9570609331130981\n",
            "\n",
            "Samples: 48100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9884154200553894\n",
            "Loss G: 1.6999993324279785\n",
            "\n",
            "Samples: 48200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9469255805015564\n",
            "Loss G: 1.684798240661621\n",
            "\n",
            "Samples: 48300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6542625427246094\n",
            "Loss G: 1.9419695138931274\n",
            "\n",
            "Samples: 48400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8358503580093384\n",
            "Loss G: 1.5539230108261108\n",
            "\n",
            "Samples: 48500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7988890409469604\n",
            "Loss G: 1.563167691230774\n",
            "\n",
            "Samples: 48600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8016237020492554\n",
            "Loss G: 1.4754711389541626\n",
            "\n",
            "Samples: 48700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.655521810054779\n",
            "Loss G: 1.6844402551651\n",
            "\n",
            "Samples: 48800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6148816347122192\n",
            "Loss G: 1.799513816833496\n",
            "\n",
            "Samples: 48900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5816619992256165\n",
            "Loss G: 2.1967287063598633\n",
            "\n",
            "Samples: 49000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5364247560501099\n",
            "Loss G: 2.361607313156128\n",
            "\n",
            "Samples: 49100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8713805079460144\n",
            "Loss G: 2.3062100410461426\n",
            "\n",
            "Samples: 49200/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.168001651763916\n",
            "Loss G: 1.7086012363433838\n",
            "\n",
            "Samples: 49300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.722166895866394\n",
            "Loss G: 1.6498130559921265\n",
            "\n",
            "Samples: 49400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7961881160736084\n",
            "Loss G: 1.5976144075393677\n",
            "\n",
            "Samples: 49500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6485980153083801\n",
            "Loss G: 1.7205570936203003\n",
            "\n",
            "Samples: 49600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.672339677810669\n",
            "Loss G: 1.7811959981918335\n",
            "\n",
            "Samples: 49700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5768594741821289\n",
            "Loss G: 1.928324580192566\n",
            "\n",
            "Samples: 49800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6072810888290405\n",
            "Loss G: 1.9675991535186768\n",
            "\n",
            "Samples: 49900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.3893302083015442\n",
            "Loss G: 2.5268187522888184\n",
            "\n",
            "Samples: 50000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4848034381866455\n",
            "Loss G: 2.6769869327545166\n",
            "\n",
            "Samples: 50100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6331546306610107\n",
            "Loss G: 2.554455041885376\n",
            "\n",
            "Samples: 50200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6553439497947693\n",
            "Loss G: 2.3434817790985107\n",
            "\n",
            "Samples: 50300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6838057041168213\n",
            "Loss G: 2.0127742290496826\n",
            "\n",
            "Samples: 50400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7330796718597412\n",
            "Loss G: 1.7233186960220337\n",
            "\n",
            "Samples: 50500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8327667117118835\n",
            "Loss G: 1.6082193851470947\n",
            "\n",
            "Samples: 50600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5242975950241089\n",
            "Loss G: 1.607538104057312\n",
            "\n",
            "Samples: 50700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6902180910110474\n",
            "Loss G: 1.6527704000473022\n",
            "\n",
            "Samples: 50800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5771042108535767\n",
            "Loss G: 1.9402974843978882\n",
            "\n",
            "Samples: 50900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6345121264457703\n",
            "Loss G: 2.288400173187256\n",
            "\n",
            "Samples: 51000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7572377920150757\n",
            "Loss G: 2.1412203311920166\n",
            "\n",
            "Samples: 51100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7082160711288452\n",
            "Loss G: 2.349773645401001\n",
            "\n",
            "Samples: 51200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7467954158782959\n",
            "Loss G: 2.0019478797912598\n",
            "\n",
            "Samples: 51300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8577154278755188\n",
            "Loss G: 1.8999775648117065\n",
            "\n",
            "Samples: 51400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7784868478775024\n",
            "Loss G: 1.5504543781280518\n",
            "\n",
            "Samples: 51500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7544582486152649\n",
            "Loss G: 1.4488484859466553\n",
            "\n",
            "Samples: 51600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7888650298118591\n",
            "Loss G: 1.4417394399642944\n",
            "\n",
            "Samples: 51700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7453069686889648\n",
            "Loss G: 1.7729449272155762\n",
            "\n",
            "Samples: 51800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8323526382446289\n",
            "Loss G: 2.272786855697632\n",
            "\n",
            "Samples: 51900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7470827102661133\n",
            "Loss G: 2.1866962909698486\n",
            "\n",
            "Samples: 52000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8851631879806519\n",
            "Loss G: 2.127185583114624\n",
            "\n",
            "Samples: 52100/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0598418712615967\n",
            "Loss G: 1.569356918334961\n",
            "\n",
            "Samples: 52200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6290320158004761\n",
            "Loss G: 1.7456470727920532\n",
            "\n",
            "Samples: 52300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8637800216674805\n",
            "Loss G: 1.3392363786697388\n",
            "\n",
            "Samples: 52400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.739995002746582\n",
            "Loss G: 1.5157926082611084\n",
            "\n",
            "Samples: 52500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6677567362785339\n",
            "Loss G: 1.6623286008834839\n",
            "\n",
            "Samples: 52600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4457615613937378\n",
            "Loss G: 1.986549973487854\n",
            "\n",
            "Samples: 52700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6353760957717896\n",
            "Loss G: 2.0384514331817627\n",
            "\n",
            "Samples: 52800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6458548307418823\n",
            "Loss G: 2.1456198692321777\n",
            "\n",
            "Samples: 52900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6553396582603455\n",
            "Loss G: 2.4110255241394043\n",
            "\n",
            "Samples: 53000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5110206604003906\n",
            "Loss G: 2.3466591835021973\n",
            "\n",
            "Samples: 53100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.554527223110199\n",
            "Loss G: 1.9641674757003784\n",
            "\n",
            "Samples: 53200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5027179718017578\n",
            "Loss G: 2.0365734100341797\n",
            "\n",
            "Samples: 53300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4755573272705078\n",
            "Loss G: 1.9551817178726196\n",
            "\n",
            "Samples: 53400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.3982160985469818\n",
            "Loss G: 2.2255890369415283\n",
            "\n",
            "Samples: 53500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.441380113363266\n",
            "Loss G: 2.1660990715026855\n",
            "\n",
            "Samples: 53600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.30015429854393005\n",
            "Loss G: 2.4180972576141357\n",
            "\n",
            "Samples: 53700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.29861509799957275\n",
            "Loss G: 2.440258502960205\n",
            "\n",
            "Samples: 53800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.31542855501174927\n",
            "Loss G: 2.6364893913269043\n",
            "\n",
            "Samples: 53900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4252018332481384\n",
            "Loss G: 2.3830995559692383\n",
            "\n",
            "Samples: 54000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.30034899711608887\n",
            "Loss G: 2.669511079788208\n",
            "\n",
            "Samples: 54100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.317740797996521\n",
            "Loss G: 2.7730255126953125\n",
            "\n",
            "Samples: 54200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.3831499218940735\n",
            "Loss G: 2.9262173175811768\n",
            "\n",
            "Samples: 54300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.3375205099582672\n",
            "Loss G: 2.6628942489624023\n",
            "\n",
            "Samples: 54400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5069022178649902\n",
            "Loss G: 2.682612657546997\n",
            "\n",
            "Samples: 54500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6552865505218506\n",
            "Loss G: 1.8562121391296387\n",
            "\n",
            "Samples: 54600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5390527248382568\n",
            "Loss G: 1.8017574548721313\n",
            "\n",
            "Samples: 54700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5626635551452637\n",
            "Loss G: 1.7705377340316772\n",
            "\n",
            "Samples: 54800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6479386687278748\n",
            "Loss G: 1.6357759237289429\n",
            "\n",
            "Samples: 54900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5380814671516418\n",
            "Loss G: 1.7713665962219238\n",
            "\n",
            "Samples: 55000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4564151167869568\n",
            "Loss G: 1.882352590560913\n",
            "\n",
            "Samples: 55100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5230821967124939\n",
            "Loss G: 2.246568202972412\n",
            "\n",
            "Samples: 55200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5315183401107788\n",
            "Loss G: 2.3931121826171875\n",
            "\n",
            "Samples: 55300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4453164339065552\n",
            "Loss G: 2.6604058742523193\n",
            "\n",
            "Samples: 55400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6507049202919006\n",
            "Loss G: 2.576012134552002\n",
            "\n",
            "Samples: 55500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5510562658309937\n",
            "Loss G: 2.1485307216644287\n",
            "\n",
            "Samples: 55600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8056451082229614\n",
            "Loss G: 1.798081874847412\n",
            "\n",
            "Samples: 55700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.902298629283905\n",
            "Loss G: 1.4904037714004517\n",
            "\n",
            "Samples: 55800/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0083701610565186\n",
            "Loss G: 1.2808033227920532\n",
            "\n",
            "Samples: 55900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9393858909606934\n",
            "Loss G: 1.2683758735656738\n",
            "\n",
            "Samples: 56000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7593404054641724\n",
            "Loss G: 1.5771859884262085\n",
            "\n",
            "Samples: 56100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7097979187965393\n",
            "Loss G: 1.9410192966461182\n",
            "\n",
            "Samples: 56200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5746028423309326\n",
            "Loss G: 2.3170716762542725\n",
            "\n",
            "Samples: 56300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7714788317680359\n",
            "Loss G: 2.527207851409912\n",
            "\n",
            "Samples: 56400/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.1323038339614868\n",
            "Loss G: 2.2412242889404297\n",
            "\n",
            "Samples: 56500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7795817852020264\n",
            "Loss G: 2.0190887451171875\n",
            "\n",
            "Samples: 56600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6086207628250122\n",
            "Loss G: 1.7745609283447266\n",
            "\n",
            "Samples: 56700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5715317726135254\n",
            "Loss G: 1.6153223514556885\n",
            "\n",
            "Samples: 56800/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0157657861709595\n",
            "Loss G: 1.533079981803894\n",
            "\n",
            "Samples: 56900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.9409948587417603\n",
            "Loss G: 1.4859919548034668\n",
            "\n",
            "Samples: 57000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8604258894920349\n",
            "Loss G: 1.4110267162322998\n",
            "\n",
            "Samples: 57100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6839994788169861\n",
            "Loss G: 1.6709256172180176\n",
            "\n",
            "Samples: 57200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5734889507293701\n",
            "Loss G: 2.0956218242645264\n",
            "\n",
            "Samples: 57300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5448804497718811\n",
            "Loss G: 2.311706304550171\n",
            "\n",
            "Samples: 57400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6889578700065613\n",
            "Loss G: 2.2986297607421875\n",
            "\n",
            "Samples: 57500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.794952929019928\n",
            "Loss G: 2.2051379680633545\n",
            "\n",
            "Samples: 57600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.591503381729126\n",
            "Loss G: 2.1487791538238525\n",
            "\n",
            "Samples: 57700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6048591136932373\n",
            "Loss G: 1.9213128089904785\n",
            "\n",
            "Samples: 57800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6680247187614441\n",
            "Loss G: 1.8105143308639526\n",
            "\n",
            "Samples: 57900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6481067538261414\n",
            "Loss G: 1.8316919803619385\n",
            "\n",
            "Samples: 58000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7654268741607666\n",
            "Loss G: 1.874048113822937\n",
            "\n",
            "Samples: 58100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6883938312530518\n",
            "Loss G: 1.8457543849945068\n",
            "\n",
            "Samples: 58200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6517771482467651\n",
            "Loss G: 2.2658989429473877\n",
            "\n",
            "Samples: 58300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5788808465003967\n",
            "Loss G: 2.5892019271850586\n",
            "\n",
            "Samples: 58400/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5877595543861389\n",
            "Loss G: 2.645024061203003\n",
            "\n",
            "Samples: 58500/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8977214694023132\n",
            "Loss G: 2.3699867725372314\n",
            "\n",
            "Samples: 58600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6236666440963745\n",
            "Loss G: 2.252638578414917\n",
            "\n",
            "Samples: 58700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6179782748222351\n",
            "Loss G: 1.8501428365707397\n",
            "\n",
            "Samples: 58800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5437299013137817\n",
            "Loss G: 1.6364073753356934\n",
            "\n",
            "Samples: 58900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.7181111574172974\n",
            "Loss G: 1.74578857421875\n",
            "\n",
            "Samples: 59000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6795896291732788\n",
            "Loss G: 1.869423747062683\n",
            "\n",
            "Samples: 59100/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5725128650665283\n",
            "Loss G: 2.1066391468048096\n",
            "\n",
            "Samples: 59200/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.4587422013282776\n",
            "Loss G: 2.550607204437256\n",
            "\n",
            "Samples: 59300/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.866889238357544\n",
            "Loss G: 2.6733415126800537\n",
            "\n",
            "Samples: 59400/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.0131163597106934\n",
            "Loss G: 2.43739652633667\n",
            "\n",
            "Samples: 59500/60000\n",
            "Epoch: 5\n",
            "Loss D: 1.318660855293274\n",
            "Loss G: 2.0820419788360596\n",
            "\n",
            "Samples: 59600/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.5812872052192688\n",
            "Loss G: 1.816542625427246\n",
            "\n",
            "Samples: 59700/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.593300998210907\n",
            "Loss G: 1.7086048126220703\n",
            "\n",
            "Samples: 59800/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6281552314758301\n",
            "Loss G: 1.7847118377685547\n",
            "\n",
            "Samples: 59900/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.8410285711288452\n",
            "Loss G: 1.6740081310272217\n",
            "\n",
            "Samples: 60000/60000\n",
            "Epoch: 5\n",
            "Loss D: 0.6463825702667236\n",
            "Loss G: 2.088118553161621\n",
            "\n",
            "Samples: 100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6693820953369141\n",
            "Loss G: 2.5313260555267334\n",
            "\n",
            "Samples: 200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7940409779548645\n",
            "Loss G: 2.74788498878479\n",
            "\n",
            "Samples: 300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.815123438835144\n",
            "Loss G: 2.50925874710083\n",
            "\n",
            "Samples: 400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7813433408737183\n",
            "Loss G: 2.315277099609375\n",
            "\n",
            "Samples: 500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7690152525901794\n",
            "Loss G: 2.1745030879974365\n",
            "\n",
            "Samples: 600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7701069116592407\n",
            "Loss G: 1.9941097497940063\n",
            "\n",
            "Samples: 700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.670997679233551\n",
            "Loss G: 1.781516671180725\n",
            "\n",
            "Samples: 800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6952064037322998\n",
            "Loss G: 1.8470121622085571\n",
            "\n",
            "Samples: 900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8021026849746704\n",
            "Loss G: 1.712121844291687\n",
            "\n",
            "Samples: 1000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5278077721595764\n",
            "Loss G: 2.5637338161468506\n",
            "\n",
            "Samples: 1100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9256016612052917\n",
            "Loss G: 2.567218065261841\n",
            "\n",
            "Samples: 1200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0976375341415405\n",
            "Loss G: 2.5230984687805176\n",
            "\n",
            "Samples: 1300/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.3565441370010376\n",
            "Loss G: 1.9531633853912354\n",
            "\n",
            "Samples: 1400/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.011436939239502\n",
            "Loss G: 1.5257292985916138\n",
            "\n",
            "Samples: 1500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8601667881011963\n",
            "Loss G: 1.1441526412963867\n",
            "\n",
            "Samples: 1600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8788645267486572\n",
            "Loss G: 1.2722026109695435\n",
            "\n",
            "Samples: 1700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9916227459907532\n",
            "Loss G: 1.463176965713501\n",
            "\n",
            "Samples: 1800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8505373597145081\n",
            "Loss G: 1.6362152099609375\n",
            "\n",
            "Samples: 1900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9763350486755371\n",
            "Loss G: 1.9260107278823853\n",
            "\n",
            "Samples: 2000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8137568831443787\n",
            "Loss G: 2.40364670753479\n",
            "\n",
            "Samples: 2100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9070301651954651\n",
            "Loss G: 2.45641827583313\n",
            "\n",
            "Samples: 2200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8409505486488342\n",
            "Loss G: 1.945123553276062\n",
            "\n",
            "Samples: 2300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7934288382530212\n",
            "Loss G: 1.6762886047363281\n",
            "\n",
            "Samples: 2400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6265780925750732\n",
            "Loss G: 1.6538457870483398\n",
            "\n",
            "Samples: 2500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.657095730304718\n",
            "Loss G: 1.6585994958877563\n",
            "\n",
            "Samples: 2600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6008215546607971\n",
            "Loss G: 1.7998210191726685\n",
            "\n",
            "Samples: 2700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5430187582969666\n",
            "Loss G: 1.9697080850601196\n",
            "\n",
            "Samples: 2800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6383442878723145\n",
            "Loss G: 2.0043656826019287\n",
            "\n",
            "Samples: 2900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6468822360038757\n",
            "Loss G: 2.3593499660491943\n",
            "\n",
            "Samples: 3000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5832384824752808\n",
            "Loss G: 2.1477298736572266\n",
            "\n",
            "Samples: 3100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6054370403289795\n",
            "Loss G: 2.388309955596924\n",
            "\n",
            "Samples: 3200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.65142422914505\n",
            "Loss G: 2.31158185005188\n",
            "\n",
            "Samples: 3300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.514731764793396\n",
            "Loss G: 2.361175537109375\n",
            "\n",
            "Samples: 3400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6411900520324707\n",
            "Loss G: 2.2380263805389404\n",
            "\n",
            "Samples: 3500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6949254274368286\n",
            "Loss G: 1.9675582647323608\n",
            "\n",
            "Samples: 3600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6181154251098633\n",
            "Loss G: 1.9703658819198608\n",
            "\n",
            "Samples: 3700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6018582582473755\n",
            "Loss G: 2.0016000270843506\n",
            "\n",
            "Samples: 3800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6232576370239258\n",
            "Loss G: 2.347012996673584\n",
            "\n",
            "Samples: 3900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5631825923919678\n",
            "Loss G: 2.4976308345794678\n",
            "\n",
            "Samples: 4000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6856274604797363\n",
            "Loss G: 2.2186944484710693\n",
            "\n",
            "Samples: 4100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6413016319274902\n",
            "Loss G: 2.3011772632598877\n",
            "\n",
            "Samples: 4200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6038007140159607\n",
            "Loss G: 2.1415703296661377\n",
            "\n",
            "Samples: 4300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6816110610961914\n",
            "Loss G: 2.0381131172180176\n",
            "\n",
            "Samples: 4400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7004072666168213\n",
            "Loss G: 2.0811514854431152\n",
            "\n",
            "Samples: 4500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6244984865188599\n",
            "Loss G: 2.5034658908843994\n",
            "\n",
            "Samples: 4600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5739112496376038\n",
            "Loss G: 2.31253719329834\n",
            "\n",
            "Samples: 4700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7936073541641235\n",
            "Loss G: 2.144904375076294\n",
            "\n",
            "Samples: 4800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.028947114944458\n",
            "Loss G: 1.9077792167663574\n",
            "\n",
            "Samples: 4900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.048480749130249\n",
            "Loss G: 1.9151535034179688\n",
            "\n",
            "Samples: 5000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5911087393760681\n",
            "Loss G: 2.05292010307312\n",
            "\n",
            "Samples: 5100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7433795928955078\n",
            "Loss G: 2.1090543270111084\n",
            "\n",
            "Samples: 5200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7113484144210815\n",
            "Loss G: 2.177344799041748\n",
            "\n",
            "Samples: 5300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.757018506526947\n",
            "Loss G: 1.9745293855667114\n",
            "\n",
            "Samples: 5400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7928519248962402\n",
            "Loss G: 1.9023979902267456\n",
            "\n",
            "Samples: 5500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9804238080978394\n",
            "Loss G: 1.4750930070877075\n",
            "\n",
            "Samples: 5600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7493118047714233\n",
            "Loss G: 1.614440679550171\n",
            "\n",
            "Samples: 5700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9946978092193604\n",
            "Loss G: 1.4051928520202637\n",
            "\n",
            "Samples: 5800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9105095267295837\n",
            "Loss G: 1.7457681894302368\n",
            "\n",
            "Samples: 5900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8046388030052185\n",
            "Loss G: 2.059642791748047\n",
            "\n",
            "Samples: 6000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8609693050384521\n",
            "Loss G: 1.9045031070709229\n",
            "\n",
            "Samples: 6100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.945165753364563\n",
            "Loss G: 1.982805848121643\n",
            "\n",
            "Samples: 6200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8925378918647766\n",
            "Loss G: 2.00315523147583\n",
            "\n",
            "Samples: 6300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8044426441192627\n",
            "Loss G: 1.6974650621414185\n",
            "\n",
            "Samples: 6400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.80318284034729\n",
            "Loss G: 1.6595933437347412\n",
            "\n",
            "Samples: 6500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6022878885269165\n",
            "Loss G: 1.7992517948150635\n",
            "\n",
            "Samples: 6600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8639394044876099\n",
            "Loss G: 1.6191234588623047\n",
            "\n",
            "Samples: 6700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5951162576675415\n",
            "Loss G: 2.0099380016326904\n",
            "\n",
            "Samples: 6800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6067443490028381\n",
            "Loss G: 2.467895984649658\n",
            "\n",
            "Samples: 6900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8720526695251465\n",
            "Loss G: 2.5271153450012207\n",
            "\n",
            "Samples: 7000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9371221661567688\n",
            "Loss G: 2.469062089920044\n",
            "\n",
            "Samples: 7100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.572596549987793\n",
            "Loss G: 2.007829427719116\n",
            "\n",
            "Samples: 7200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6089866757392883\n",
            "Loss G: 1.8394415378570557\n",
            "\n",
            "Samples: 7300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.822013258934021\n",
            "Loss G: 1.6080291271209717\n",
            "\n",
            "Samples: 7400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7643988132476807\n",
            "Loss G: 1.3079878091812134\n",
            "\n",
            "Samples: 7500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.633395791053772\n",
            "Loss G: 1.4388684034347534\n",
            "\n",
            "Samples: 7600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.49911919236183167\n",
            "Loss G: 1.8908770084381104\n",
            "\n",
            "Samples: 7700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5048574209213257\n",
            "Loss G: 2.343289852142334\n",
            "\n",
            "Samples: 7800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7329112887382507\n",
            "Loss G: 2.51302433013916\n",
            "\n",
            "Samples: 7900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5490405559539795\n",
            "Loss G: 2.4406485557556152\n",
            "\n",
            "Samples: 8000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8542407155036926\n",
            "Loss G: 2.3172130584716797\n",
            "\n",
            "Samples: 8100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5725073218345642\n",
            "Loss G: 2.3024091720581055\n",
            "\n",
            "Samples: 8200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.45778679847717285\n",
            "Loss G: 1.8823989629745483\n",
            "\n",
            "Samples: 8300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5591233372688293\n",
            "Loss G: 1.8007994890213013\n",
            "\n",
            "Samples: 8400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6275185346603394\n",
            "Loss G: 1.505013108253479\n",
            "\n",
            "Samples: 8500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8704888820648193\n",
            "Loss G: 1.2853858470916748\n",
            "\n",
            "Samples: 8600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5689946413040161\n",
            "Loss G: 1.683026671409607\n",
            "\n",
            "Samples: 8700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.679110050201416\n",
            "Loss G: 1.7581084966659546\n",
            "\n",
            "Samples: 8800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7700817584991455\n",
            "Loss G: 1.8735129833221436\n",
            "\n",
            "Samples: 8900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7920596599578857\n",
            "Loss G: 2.1727688312530518\n",
            "\n",
            "Samples: 9000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.46583425998687744\n",
            "Loss G: 2.420006275177002\n",
            "\n",
            "Samples: 9100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4776932895183563\n",
            "Loss G: 2.1042678356170654\n",
            "\n",
            "Samples: 9200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.693469762802124\n",
            "Loss G: 2.298128843307495\n",
            "\n",
            "Samples: 9300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6622580885887146\n",
            "Loss G: 2.4899003505706787\n",
            "\n",
            "Samples: 9400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6224132776260376\n",
            "Loss G: 1.9284197092056274\n",
            "\n",
            "Samples: 9500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4737153649330139\n",
            "Loss G: 1.8626130819320679\n",
            "\n",
            "Samples: 9600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.675234317779541\n",
            "Loss G: 1.5787421464920044\n",
            "\n",
            "Samples: 9700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5799362659454346\n",
            "Loss G: 1.848172903060913\n",
            "\n",
            "Samples: 9800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5760066509246826\n",
            "Loss G: 2.166686773300171\n",
            "\n",
            "Samples: 9900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5247375965118408\n",
            "Loss G: 2.165816307067871\n",
            "\n",
            "Samples: 10000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6085863709449768\n",
            "Loss G: 2.308140754699707\n",
            "\n",
            "Samples: 10100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7775909900665283\n",
            "Loss G: 1.958406686782837\n",
            "\n",
            "Samples: 10200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8477670550346375\n",
            "Loss G: 1.6791424751281738\n",
            "\n",
            "Samples: 10300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9588505625724792\n",
            "Loss G: 1.2099794149398804\n",
            "\n",
            "Samples: 10400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6297293305397034\n",
            "Loss G: 1.4963957071304321\n",
            "\n",
            "Samples: 10500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6065333485603333\n",
            "Loss G: 1.8938062191009521\n",
            "\n",
            "Samples: 10600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5817080736160278\n",
            "Loss G: 2.188744068145752\n",
            "\n",
            "Samples: 10700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.729804277420044\n",
            "Loss G: 2.4104485511779785\n",
            "\n",
            "Samples: 10800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8533536195755005\n",
            "Loss G: 2.4076528549194336\n",
            "\n",
            "Samples: 10900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7328327298164368\n",
            "Loss G: 2.0539262294769287\n",
            "\n",
            "Samples: 11000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8658213019371033\n",
            "Loss G: 2.064828634262085\n",
            "\n",
            "Samples: 11100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7425572872161865\n",
            "Loss G: 1.8006161451339722\n",
            "\n",
            "Samples: 11200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8821816444396973\n",
            "Loss G: 1.424653172492981\n",
            "\n",
            "Samples: 11300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.753758430480957\n",
            "Loss G: 1.6707308292388916\n",
            "\n",
            "Samples: 11400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6424257159233093\n",
            "Loss G: 1.9582653045654297\n",
            "\n",
            "Samples: 11500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6602514982223511\n",
            "Loss G: 2.1673219203948975\n",
            "\n",
            "Samples: 11600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6593517065048218\n",
            "Loss G: 2.1146981716156006\n",
            "\n",
            "Samples: 11700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6211720705032349\n",
            "Loss G: 2.37241268157959\n",
            "\n",
            "Samples: 11800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7583046555519104\n",
            "Loss G: 2.0126707553863525\n",
            "\n",
            "Samples: 11900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8428648710250854\n",
            "Loss G: 1.66434907913208\n",
            "\n",
            "Samples: 12000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8341063857078552\n",
            "Loss G: 1.5615090131759644\n",
            "\n",
            "Samples: 12100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7741473913192749\n",
            "Loss G: 1.5739327669143677\n",
            "\n",
            "Samples: 12200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8332963585853577\n",
            "Loss G: 1.701084852218628\n",
            "\n",
            "Samples: 12300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6141122579574585\n",
            "Loss G: 2.2623205184936523\n",
            "\n",
            "Samples: 12400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6857655048370361\n",
            "Loss G: 2.1798224449157715\n",
            "\n",
            "Samples: 12500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5371043086051941\n",
            "Loss G: 2.3888983726501465\n",
            "\n",
            "Samples: 12600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5686386823654175\n",
            "Loss G: 2.7184503078460693\n",
            "\n",
            "Samples: 12700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7071817517280579\n",
            "Loss G: 2.2015042304992676\n",
            "\n",
            "Samples: 12800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6118399500846863\n",
            "Loss G: 2.3251402378082275\n",
            "\n",
            "Samples: 12900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6399635076522827\n",
            "Loss G: 2.320045232772827\n",
            "\n",
            "Samples: 13000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9008669853210449\n",
            "Loss G: 1.9439449310302734\n",
            "\n",
            "Samples: 13100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.779313325881958\n",
            "Loss G: 1.5829888582229614\n",
            "\n",
            "Samples: 13200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7969030141830444\n",
            "Loss G: 1.3515104055404663\n",
            "\n",
            "Samples: 13300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.73117995262146\n",
            "Loss G: 1.5447014570236206\n",
            "\n",
            "Samples: 13400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5640016794204712\n",
            "Loss G: 1.8808495998382568\n",
            "\n",
            "Samples: 13500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.464851438999176\n",
            "Loss G: 2.4671590328216553\n",
            "\n",
            "Samples: 13600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7090779542922974\n",
            "Loss G: 2.344926595687866\n",
            "\n",
            "Samples: 13700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8452391624450684\n",
            "Loss G: 2.4802212715148926\n",
            "\n",
            "Samples: 13800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7910977005958557\n",
            "Loss G: 2.4207892417907715\n",
            "\n",
            "Samples: 13900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6594799757003784\n",
            "Loss G: 2.070415496826172\n",
            "\n",
            "Samples: 14000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5870427489280701\n",
            "Loss G: 2.0866236686706543\n",
            "\n",
            "Samples: 14100/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1566416025161743\n",
            "Loss G: 1.54996657371521\n",
            "\n",
            "Samples: 14200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1329790353775024\n",
            "Loss G: 1.527734637260437\n",
            "\n",
            "Samples: 14300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9009687900543213\n",
            "Loss G: 1.7105916738510132\n",
            "\n",
            "Samples: 14400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8584128618240356\n",
            "Loss G: 1.709538221359253\n",
            "\n",
            "Samples: 14500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7180067896842957\n",
            "Loss G: 1.7427523136138916\n",
            "\n",
            "Samples: 14600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7247203588485718\n",
            "Loss G: 2.1827783584594727\n",
            "\n",
            "Samples: 14700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8538460731506348\n",
            "Loss G: 2.2786660194396973\n",
            "\n",
            "Samples: 14800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8082619309425354\n",
            "Loss G: 2.1477458477020264\n",
            "\n",
            "Samples: 14900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7814962267875671\n",
            "Loss G: 2.1030330657958984\n",
            "\n",
            "Samples: 15000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8588943481445312\n",
            "Loss G: 2.022876501083374\n",
            "\n",
            "Samples: 15100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6369661092758179\n",
            "Loss G: 1.7070064544677734\n",
            "\n",
            "Samples: 15200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5781643986701965\n",
            "Loss G: 2.0373170375823975\n",
            "\n",
            "Samples: 15300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5374586582183838\n",
            "Loss G: 1.851124882698059\n",
            "\n",
            "Samples: 15400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5053961277008057\n",
            "Loss G: 1.901161789894104\n",
            "\n",
            "Samples: 15500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6740190386772156\n",
            "Loss G: 2.0964431762695312\n",
            "\n",
            "Samples: 15600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.642662525177002\n",
            "Loss G: 2.4299521446228027\n",
            "\n",
            "Samples: 15700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5794273614883423\n",
            "Loss G: 2.425868272781372\n",
            "\n",
            "Samples: 15800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5337991714477539\n",
            "Loss G: 2.530275583267212\n",
            "\n",
            "Samples: 15900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5950303673744202\n",
            "Loss G: 2.2804343700408936\n",
            "\n",
            "Samples: 16000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9348970651626587\n",
            "Loss G: 2.161792755126953\n",
            "\n",
            "Samples: 16100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7008345127105713\n",
            "Loss G: 1.862301230430603\n",
            "\n",
            "Samples: 16200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7769430875778198\n",
            "Loss G: 1.566439151763916\n",
            "\n",
            "Samples: 16300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7756854295730591\n",
            "Loss G: 1.4400405883789062\n",
            "\n",
            "Samples: 16400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5988273024559021\n",
            "Loss G: 1.8298723697662354\n",
            "\n",
            "Samples: 16500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6172767877578735\n",
            "Loss G: 2.050902843475342\n",
            "\n",
            "Samples: 16600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6628221869468689\n",
            "Loss G: 2.096937894821167\n",
            "\n",
            "Samples: 16700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7578326463699341\n",
            "Loss G: 2.225416898727417\n",
            "\n",
            "Samples: 16800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9884275197982788\n",
            "Loss G: 1.9637463092803955\n",
            "\n",
            "Samples: 16900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9165030717849731\n",
            "Loss G: 2.049325704574585\n",
            "\n",
            "Samples: 17000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8060097694396973\n",
            "Loss G: 1.8682399988174438\n",
            "\n",
            "Samples: 17100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8570175170898438\n",
            "Loss G: 1.4298837184906006\n",
            "\n",
            "Samples: 17200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0115091800689697\n",
            "Loss G: 1.434505581855774\n",
            "\n",
            "Samples: 17300/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0491482019424438\n",
            "Loss G: 1.5558937788009644\n",
            "\n",
            "Samples: 17400/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0385229587554932\n",
            "Loss G: 1.4260927438735962\n",
            "\n",
            "Samples: 17500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9576214551925659\n",
            "Loss G: 1.750112533569336\n",
            "\n",
            "Samples: 17600/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.14109206199646\n",
            "Loss G: 1.9688750505447388\n",
            "\n",
            "Samples: 17700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9276931881904602\n",
            "Loss G: 2.197178840637207\n",
            "\n",
            "Samples: 17800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7047640085220337\n",
            "Loss G: 1.920859694480896\n",
            "\n",
            "Samples: 17900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.640102207660675\n",
            "Loss G: 2.124481678009033\n",
            "\n",
            "Samples: 18000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8499317765235901\n",
            "Loss G: 2.206592082977295\n",
            "\n",
            "Samples: 18100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7492375373840332\n",
            "Loss G: 2.000916004180908\n",
            "\n",
            "Samples: 18200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.49660778045654297\n",
            "Loss G: 2.1616101264953613\n",
            "\n",
            "Samples: 18300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5624071955680847\n",
            "Loss G: 2.0379204750061035\n",
            "\n",
            "Samples: 18400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5113667845726013\n",
            "Loss G: 2.314499855041504\n",
            "\n",
            "Samples: 18500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6209691762924194\n",
            "Loss G: 2.2219178676605225\n",
            "\n",
            "Samples: 18600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8596282005310059\n",
            "Loss G: 2.5721981525421143\n",
            "\n",
            "Samples: 18700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8236280679702759\n",
            "Loss G: 2.3163647651672363\n",
            "\n",
            "Samples: 18800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6814274787902832\n",
            "Loss G: 2.489140510559082\n",
            "\n",
            "Samples: 18900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4325781464576721\n",
            "Loss G: 2.8458800315856934\n",
            "\n",
            "Samples: 19000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.43857115507125854\n",
            "Loss G: 2.6084954738616943\n",
            "\n",
            "Samples: 19100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6442664861679077\n",
            "Loss G: 2.441377878189087\n",
            "\n",
            "Samples: 19200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5334660410881042\n",
            "Loss G: 2.4751532077789307\n",
            "\n",
            "Samples: 19300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5783388614654541\n",
            "Loss G: 2.2728171348571777\n",
            "\n",
            "Samples: 19400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8143160343170166\n",
            "Loss G: 2.1726913452148438\n",
            "\n",
            "Samples: 19500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7295113801956177\n",
            "Loss G: 2.1998679637908936\n",
            "\n",
            "Samples: 19600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.490425169467926\n",
            "Loss G: 2.5290842056274414\n",
            "\n",
            "Samples: 19700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5410918593406677\n",
            "Loss G: 2.5374341011047363\n",
            "\n",
            "Samples: 19800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7386846542358398\n",
            "Loss G: 2.5829873085021973\n",
            "\n",
            "Samples: 19900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8529220819473267\n",
            "Loss G: 2.6652543544769287\n",
            "\n",
            "Samples: 20000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7293309569358826\n",
            "Loss G: 2.7470622062683105\n",
            "\n",
            "Samples: 20100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6364834308624268\n",
            "Loss G: 2.5248217582702637\n",
            "\n",
            "Samples: 20200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6990055441856384\n",
            "Loss G: 2.383807420730591\n",
            "\n",
            "Samples: 20300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5513153076171875\n",
            "Loss G: 2.7707841396331787\n",
            "\n",
            "Samples: 20400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5086246728897095\n",
            "Loss G: 2.845937728881836\n",
            "\n",
            "Samples: 20500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4958531856536865\n",
            "Loss G: 2.5212090015411377\n",
            "\n",
            "Samples: 20600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.623228132724762\n",
            "Loss G: 2.121434450149536\n",
            "\n",
            "Samples: 20700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4719940423965454\n",
            "Loss G: 2.2232489585876465\n",
            "\n",
            "Samples: 20800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5478163957595825\n",
            "Loss G: 2.2982308864593506\n",
            "\n",
            "Samples: 20900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6218169927597046\n",
            "Loss G: 2.256728410720825\n",
            "\n",
            "Samples: 21000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5847223997116089\n",
            "Loss G: 2.1759183406829834\n",
            "\n",
            "Samples: 21100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5703988671302795\n",
            "Loss G: 2.3977982997894287\n",
            "\n",
            "Samples: 21200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6732862591743469\n",
            "Loss G: 2.4420039653778076\n",
            "\n",
            "Samples: 21300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5854396820068359\n",
            "Loss G: 2.5445830821990967\n",
            "\n",
            "Samples: 21400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6780713796615601\n",
            "Loss G: 2.4244399070739746\n",
            "\n",
            "Samples: 21500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.505075216293335\n",
            "Loss G: 2.362308979034424\n",
            "\n",
            "Samples: 21600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5780197978019714\n",
            "Loss G: 2.6895952224731445\n",
            "\n",
            "Samples: 21700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5916758179664612\n",
            "Loss G: 2.349496603012085\n",
            "\n",
            "Samples: 21800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6835064888000488\n",
            "Loss G: 2.2049567699432373\n",
            "\n",
            "Samples: 21900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8989025354385376\n",
            "Loss G: 1.7487890720367432\n",
            "\n",
            "Samples: 22000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7228120565414429\n",
            "Loss G: 2.0775794982910156\n",
            "\n",
            "Samples: 22100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8533756732940674\n",
            "Loss G: 1.8352128267288208\n",
            "\n",
            "Samples: 22200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9105249047279358\n",
            "Loss G: 1.87680983543396\n",
            "\n",
            "Samples: 22300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6439779996871948\n",
            "Loss G: 1.9414315223693848\n",
            "\n",
            "Samples: 22400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6238720417022705\n",
            "Loss G: 2.129565477371216\n",
            "\n",
            "Samples: 22500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7990748882293701\n",
            "Loss G: 2.006230115890503\n",
            "\n",
            "Samples: 22600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8768957853317261\n",
            "Loss G: 2.0206010341644287\n",
            "\n",
            "Samples: 22700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7439111471176147\n",
            "Loss G: 1.878988265991211\n",
            "\n",
            "Samples: 22800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8857314586639404\n",
            "Loss G: 1.722510814666748\n",
            "\n",
            "Samples: 22900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8039236068725586\n",
            "Loss G: 1.472269892692566\n",
            "\n",
            "Samples: 23000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8693267107009888\n",
            "Loss G: 1.6489232778549194\n",
            "\n",
            "Samples: 23100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8061623573303223\n",
            "Loss G: 1.9712704420089722\n",
            "\n",
            "Samples: 23200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8550347089767456\n",
            "Loss G: 2.0424764156341553\n",
            "\n",
            "Samples: 23300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8993281722068787\n",
            "Loss G: 1.9616127014160156\n",
            "\n",
            "Samples: 23400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.66808021068573\n",
            "Loss G: 2.418935775756836\n",
            "\n",
            "Samples: 23500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6590562462806702\n",
            "Loss G: 2.1231350898742676\n",
            "\n",
            "Samples: 23600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5834765434265137\n",
            "Loss G: 2.301797389984131\n",
            "\n",
            "Samples: 23700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6217719316482544\n",
            "Loss G: 2.295706272125244\n",
            "\n",
            "Samples: 23800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5700488686561584\n",
            "Loss G: 1.9710203409194946\n",
            "\n",
            "Samples: 23900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6394177079200745\n",
            "Loss G: 2.292253255844116\n",
            "\n",
            "Samples: 24000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5647391080856323\n",
            "Loss G: 2.177530527114868\n",
            "\n",
            "Samples: 24100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6376218199729919\n",
            "Loss G: 2.2365102767944336\n",
            "\n",
            "Samples: 24200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5439239144325256\n",
            "Loss G: 1.9931707382202148\n",
            "\n",
            "Samples: 24300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.521614134311676\n",
            "Loss G: 1.974221110343933\n",
            "\n",
            "Samples: 24400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5695744752883911\n",
            "Loss G: 1.7640622854232788\n",
            "\n",
            "Samples: 24500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4912060499191284\n",
            "Loss G: 2.513979434967041\n",
            "\n",
            "Samples: 24600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4707014560699463\n",
            "Loss G: 2.355865478515625\n",
            "\n",
            "Samples: 24700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.620063066482544\n",
            "Loss G: 2.548170804977417\n",
            "\n",
            "Samples: 24800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5615371465682983\n",
            "Loss G: 2.2739813327789307\n",
            "\n",
            "Samples: 24900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4720321595668793\n",
            "Loss G: 2.600999116897583\n",
            "\n",
            "Samples: 25000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4982111155986786\n",
            "Loss G: 1.9186221361160278\n",
            "\n",
            "Samples: 25100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4753473103046417\n",
            "Loss G: 1.8770408630371094\n",
            "\n",
            "Samples: 25200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5320504903793335\n",
            "Loss G: 2.088010311126709\n",
            "\n",
            "Samples: 25300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5966520309448242\n",
            "Loss G: 1.767574429512024\n",
            "\n",
            "Samples: 25400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9542349576950073\n",
            "Loss G: 1.693207025527954\n",
            "\n",
            "Samples: 25500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.794686496257782\n",
            "Loss G: 1.6873021125793457\n",
            "\n",
            "Samples: 25600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7548832297325134\n",
            "Loss G: 1.5071536302566528\n",
            "\n",
            "Samples: 25700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8237659931182861\n",
            "Loss G: 1.8424595594406128\n",
            "\n",
            "Samples: 25800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6341627836227417\n",
            "Loss G: 1.768290638923645\n",
            "\n",
            "Samples: 25900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.67533940076828\n",
            "Loss G: 2.0506749153137207\n",
            "\n",
            "Samples: 26000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7694387435913086\n",
            "Loss G: 1.8610235452651978\n",
            "\n",
            "Samples: 26100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8214592933654785\n",
            "Loss G: 1.7365249395370483\n",
            "\n",
            "Samples: 26200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7719393968582153\n",
            "Loss G: 1.5670868158340454\n",
            "\n",
            "Samples: 26300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.835230827331543\n",
            "Loss G: 1.5741969347000122\n",
            "\n",
            "Samples: 26400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7900679111480713\n",
            "Loss G: 1.4420166015625\n",
            "\n",
            "Samples: 26500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7047957181930542\n",
            "Loss G: 1.6035009622573853\n",
            "\n",
            "Samples: 26600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8534940481185913\n",
            "Loss G: 1.9198315143585205\n",
            "\n",
            "Samples: 26700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7885314226150513\n",
            "Loss G: 1.9554357528686523\n",
            "\n",
            "Samples: 26800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.123274326324463\n",
            "Loss G: 1.9267752170562744\n",
            "\n",
            "Samples: 26900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9585781097412109\n",
            "Loss G: 1.5943516492843628\n",
            "\n",
            "Samples: 27000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7182060480117798\n",
            "Loss G: 1.5696263313293457\n",
            "\n",
            "Samples: 27100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7955636978149414\n",
            "Loss G: 1.5438015460968018\n",
            "\n",
            "Samples: 27200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.026606559753418\n",
            "Loss G: 1.5452308654785156\n",
            "\n",
            "Samples: 27300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9950172901153564\n",
            "Loss G: 1.490490436553955\n",
            "\n",
            "Samples: 27400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9733868837356567\n",
            "Loss G: 1.376392126083374\n",
            "\n",
            "Samples: 27500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8619602918624878\n",
            "Loss G: 1.6358654499053955\n",
            "\n",
            "Samples: 27600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6764765977859497\n",
            "Loss G: 1.7654740810394287\n",
            "\n",
            "Samples: 27700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7072089910507202\n",
            "Loss G: 1.6116247177124023\n",
            "\n",
            "Samples: 27800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7412949800491333\n",
            "Loss G: 1.9673635959625244\n",
            "\n",
            "Samples: 27900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6261072158813477\n",
            "Loss G: 2.1931915283203125\n",
            "\n",
            "Samples: 28000/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1392600536346436\n",
            "Loss G: 1.88655424118042\n",
            "\n",
            "Samples: 28100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8949066996574402\n",
            "Loss G: 1.8297746181488037\n",
            "\n",
            "Samples: 28200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.898103654384613\n",
            "Loss G: 1.359039068222046\n",
            "\n",
            "Samples: 28300/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.002560019493103\n",
            "Loss G: 1.243857502937317\n",
            "\n",
            "Samples: 28400/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1330995559692383\n",
            "Loss G: 1.3222429752349854\n",
            "\n",
            "Samples: 28500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9246518611907959\n",
            "Loss G: 1.5128527879714966\n",
            "\n",
            "Samples: 28600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9078494310379028\n",
            "Loss G: 1.7404210567474365\n",
            "\n",
            "Samples: 28700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9404553771018982\n",
            "Loss G: 1.8410513401031494\n",
            "\n",
            "Samples: 28800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6651807427406311\n",
            "Loss G: 2.4276390075683594\n",
            "\n",
            "Samples: 28900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7223086357116699\n",
            "Loss G: 2.2096426486968994\n",
            "\n",
            "Samples: 29000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7782822847366333\n",
            "Loss G: 2.08085560798645\n",
            "\n",
            "Samples: 29100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9178050756454468\n",
            "Loss G: 1.6810516119003296\n",
            "\n",
            "Samples: 29200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.078984022140503\n",
            "Loss G: 1.6455248594284058\n",
            "\n",
            "Samples: 29300/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0208629369735718\n",
            "Loss G: 1.6349585056304932\n",
            "\n",
            "Samples: 29400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8815306425094604\n",
            "Loss G: 1.6065877676010132\n",
            "\n",
            "Samples: 29500/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0178145170211792\n",
            "Loss G: 1.3492629528045654\n",
            "\n",
            "Samples: 29600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9241513013839722\n",
            "Loss G: 1.4812488555908203\n",
            "\n",
            "Samples: 29700/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0243430137634277\n",
            "Loss G: 1.3564921617507935\n",
            "\n",
            "Samples: 29800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0500646829605103\n",
            "Loss G: 1.4930105209350586\n",
            "\n",
            "Samples: 29900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1795861721038818\n",
            "Loss G: 1.7637189626693726\n",
            "\n",
            "Samples: 30000/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0997140407562256\n",
            "Loss G: 1.4885014295578003\n",
            "\n",
            "Samples: 30100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8428025245666504\n",
            "Loss G: 1.8067775964736938\n",
            "\n",
            "Samples: 30200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9124802350997925\n",
            "Loss G: 1.7511337995529175\n",
            "\n",
            "Samples: 30300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8764760494232178\n",
            "Loss G: 1.6623693704605103\n",
            "\n",
            "Samples: 30400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9691704511642456\n",
            "Loss G: 1.581158995628357\n",
            "\n",
            "Samples: 30500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8394678235054016\n",
            "Loss G: 1.5018831491470337\n",
            "\n",
            "Samples: 30600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9135319590568542\n",
            "Loss G: 1.3820143938064575\n",
            "\n",
            "Samples: 30700/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.103596568107605\n",
            "Loss G: 1.2524138689041138\n",
            "\n",
            "Samples: 30800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9564609527587891\n",
            "Loss G: 1.4891974925994873\n",
            "\n",
            "Samples: 30900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7845664024353027\n",
            "Loss G: 1.5282942056655884\n",
            "\n",
            "Samples: 31000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8395435214042664\n",
            "Loss G: 1.718302845954895\n",
            "\n",
            "Samples: 31100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.848120927810669\n",
            "Loss G: 1.7385036945343018\n",
            "\n",
            "Samples: 31200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.064795732498169\n",
            "Loss G: 1.5211611986160278\n",
            "\n",
            "Samples: 31300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7985121011734009\n",
            "Loss G: 1.6618130207061768\n",
            "\n",
            "Samples: 31400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9265468120574951\n",
            "Loss G: 1.6080677509307861\n",
            "\n",
            "Samples: 31500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9991859197616577\n",
            "Loss G: 1.520064353942871\n",
            "\n",
            "Samples: 31600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9211142659187317\n",
            "Loss G: 1.5399023294448853\n",
            "\n",
            "Samples: 31700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7937616109848022\n",
            "Loss G: 1.4602516889572144\n",
            "\n",
            "Samples: 31800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9290128350257874\n",
            "Loss G: 1.7247884273529053\n",
            "\n",
            "Samples: 31900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0200520753860474\n",
            "Loss G: 1.7785249948501587\n",
            "\n",
            "Samples: 32000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8966692686080933\n",
            "Loss G: 1.862976312637329\n",
            "\n",
            "Samples: 32100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9047262668609619\n",
            "Loss G: 1.6990405321121216\n",
            "\n",
            "Samples: 32200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.735985517501831\n",
            "Loss G: 1.4381929636001587\n",
            "\n",
            "Samples: 32300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7508670091629028\n",
            "Loss G: 1.380462646484375\n",
            "\n",
            "Samples: 32400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9174737930297852\n",
            "Loss G: 1.390529751777649\n",
            "\n",
            "Samples: 32500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8228625655174255\n",
            "Loss G: 1.4948147535324097\n",
            "\n",
            "Samples: 32600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.799667477607727\n",
            "Loss G: 1.6569979190826416\n",
            "\n",
            "Samples: 32700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8607357144355774\n",
            "Loss G: 1.5864967107772827\n",
            "\n",
            "Samples: 32800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9100271463394165\n",
            "Loss G: 1.7070035934448242\n",
            "\n",
            "Samples: 32900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7582722902297974\n",
            "Loss G: 1.8461955785751343\n",
            "\n",
            "Samples: 33000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4660399258136749\n",
            "Loss G: 2.0320990085601807\n",
            "\n",
            "Samples: 33100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7286189794540405\n",
            "Loss G: 1.8177344799041748\n",
            "\n",
            "Samples: 33200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8304957151412964\n",
            "Loss G: 1.5953725576400757\n",
            "\n",
            "Samples: 33300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7069615125656128\n",
            "Loss G: 1.523348093032837\n",
            "\n",
            "Samples: 33400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6948128938674927\n",
            "Loss G: 1.4220002889633179\n",
            "\n",
            "Samples: 33500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7412863969802856\n",
            "Loss G: 1.5953872203826904\n",
            "\n",
            "Samples: 33600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7120929956436157\n",
            "Loss G: 1.67011559009552\n",
            "\n",
            "Samples: 33700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8422088623046875\n",
            "Loss G: 1.850999355316162\n",
            "\n",
            "Samples: 33800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0404300689697266\n",
            "Loss G: 1.7019002437591553\n",
            "\n",
            "Samples: 33900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0905425548553467\n",
            "Loss G: 1.524976372718811\n",
            "\n",
            "Samples: 34000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7339380979537964\n",
            "Loss G: 1.2592456340789795\n",
            "\n",
            "Samples: 34100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6412167549133301\n",
            "Loss G: 1.2085249423980713\n",
            "\n",
            "Samples: 34200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7445305585861206\n",
            "Loss G: 1.3532299995422363\n",
            "\n",
            "Samples: 34300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5773178935050964\n",
            "Loss G: 1.7420612573623657\n",
            "\n",
            "Samples: 34400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8448885679244995\n",
            "Loss G: 1.9788100719451904\n",
            "\n",
            "Samples: 34500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8665516972541809\n",
            "Loss G: 2.1813247203826904\n",
            "\n",
            "Samples: 34600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8177915215492249\n",
            "Loss G: 1.9642107486724854\n",
            "\n",
            "Samples: 34700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9363809823989868\n",
            "Loss G: 1.5804036855697632\n",
            "\n",
            "Samples: 34800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8494365215301514\n",
            "Loss G: 1.4312418699264526\n",
            "\n",
            "Samples: 34900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9556232690811157\n",
            "Loss G: 1.3244847059249878\n",
            "\n",
            "Samples: 35000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8186835050582886\n",
            "Loss G: 1.3260475397109985\n",
            "\n",
            "Samples: 35100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9837083220481873\n",
            "Loss G: 1.437940001487732\n",
            "\n",
            "Samples: 35200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7197881937026978\n",
            "Loss G: 1.69456148147583\n",
            "\n",
            "Samples: 35300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5104261040687561\n",
            "Loss G: 2.1124472618103027\n",
            "\n",
            "Samples: 35400/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0504661798477173\n",
            "Loss G: 1.9727634191513062\n",
            "\n",
            "Samples: 35500/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0878956317901611\n",
            "Loss G: 1.9200289249420166\n",
            "\n",
            "Samples: 35600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7659589052200317\n",
            "Loss G: 1.7489289045333862\n",
            "\n",
            "Samples: 35700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7055325508117676\n",
            "Loss G: 1.6597225666046143\n",
            "\n",
            "Samples: 35800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8868294954299927\n",
            "Loss G: 1.5349133014678955\n",
            "\n",
            "Samples: 35900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8302650451660156\n",
            "Loss G: 1.4418777227401733\n",
            "\n",
            "Samples: 36000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7991030216217041\n",
            "Loss G: 1.7913818359375\n",
            "\n",
            "Samples: 36100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.849716305732727\n",
            "Loss G: 1.876859426498413\n",
            "\n",
            "Samples: 36200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.2112202644348145\n",
            "Loss G: 2.0883853435516357\n",
            "\n",
            "Samples: 36300/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.5635775327682495\n",
            "Loss G: 1.99590265750885\n",
            "\n",
            "Samples: 36400/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.145890712738037\n",
            "Loss G: 1.800973892211914\n",
            "\n",
            "Samples: 36500/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0864534378051758\n",
            "Loss G: 1.3550204038619995\n",
            "\n",
            "Samples: 36600/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1105587482452393\n",
            "Loss G: 1.2194366455078125\n",
            "\n",
            "Samples: 36700/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.5182273387908936\n",
            "Loss G: 0.9943974018096924\n",
            "\n",
            "Samples: 36800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.4253607988357544\n",
            "Loss G: 1.043864369392395\n",
            "\n",
            "Samples: 36900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.477357029914856\n",
            "Loss G: 1.0515886545181274\n",
            "\n",
            "Samples: 37000/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1017180681228638\n",
            "Loss G: 1.583612322807312\n",
            "\n",
            "Samples: 37100/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1830732822418213\n",
            "Loss G: 1.860689640045166\n",
            "\n",
            "Samples: 37200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.3062632083892822\n",
            "Loss G: 1.8456199169158936\n",
            "\n",
            "Samples: 37300/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1500880718231201\n",
            "Loss G: 1.957410216331482\n",
            "\n",
            "Samples: 37400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9281944036483765\n",
            "Loss G: 1.8140718936920166\n",
            "\n",
            "Samples: 37500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.883882999420166\n",
            "Loss G: 1.7824459075927734\n",
            "\n",
            "Samples: 37600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7869534492492676\n",
            "Loss G: 1.879166841506958\n",
            "\n",
            "Samples: 37700/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0181713104248047\n",
            "Loss G: 1.6214337348937988\n",
            "\n",
            "Samples: 37800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9437971115112305\n",
            "Loss G: 2.0344467163085938\n",
            "\n",
            "Samples: 37900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9622927904129028\n",
            "Loss G: 1.8940361738204956\n",
            "\n",
            "Samples: 38000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7429840564727783\n",
            "Loss G: 2.1251847743988037\n",
            "\n",
            "Samples: 38100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.735937237739563\n",
            "Loss G: 2.3178250789642334\n",
            "\n",
            "Samples: 38200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9132968187332153\n",
            "Loss G: 2.20283842086792\n",
            "\n",
            "Samples: 38300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9621350169181824\n",
            "Loss G: 1.9911507368087769\n",
            "\n",
            "Samples: 38400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8581864833831787\n",
            "Loss G: 1.7075326442718506\n",
            "\n",
            "Samples: 38500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8463168144226074\n",
            "Loss G: 1.3055850267410278\n",
            "\n",
            "Samples: 38600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7714332342147827\n",
            "Loss G: 1.6258611679077148\n",
            "\n",
            "Samples: 38700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6857537031173706\n",
            "Loss G: 1.6411892175674438\n",
            "\n",
            "Samples: 38800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7843781113624573\n",
            "Loss G: 1.7222448587417603\n",
            "\n",
            "Samples: 38900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6932495832443237\n",
            "Loss G: 2.0427181720733643\n",
            "\n",
            "Samples: 39000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.693617045879364\n",
            "Loss G: 2.113959789276123\n",
            "\n",
            "Samples: 39100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8889690637588501\n",
            "Loss G: 2.1748480796813965\n",
            "\n",
            "Samples: 39200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6475364565849304\n",
            "Loss G: 2.114319324493408\n",
            "\n",
            "Samples: 39300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8345571756362915\n",
            "Loss G: 1.7056609392166138\n",
            "\n",
            "Samples: 39400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7937654256820679\n",
            "Loss G: 1.6916518211364746\n",
            "\n",
            "Samples: 39500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7983237504959106\n",
            "Loss G: 1.3494521379470825\n",
            "\n",
            "Samples: 39600/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0374343395233154\n",
            "Loss G: 1.40324068069458\n",
            "\n",
            "Samples: 39700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.934576153755188\n",
            "Loss G: 1.562509298324585\n",
            "\n",
            "Samples: 39800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1218289136886597\n",
            "Loss G: 1.6440006494522095\n",
            "\n",
            "Samples: 39900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.2277357578277588\n",
            "Loss G: 1.4934366941452026\n",
            "\n",
            "Samples: 40000/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.061922311782837\n",
            "Loss G: 1.6973356008529663\n",
            "\n",
            "Samples: 40100/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0309133529663086\n",
            "Loss G: 1.4858593940734863\n",
            "\n",
            "Samples: 40200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8914627432823181\n",
            "Loss G: 1.574859857559204\n",
            "\n",
            "Samples: 40300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.785915732383728\n",
            "Loss G: 1.6656429767608643\n",
            "\n",
            "Samples: 40400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9498443603515625\n",
            "Loss G: 1.6269910335540771\n",
            "\n",
            "Samples: 40500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9084231853485107\n",
            "Loss G: 1.5475220680236816\n",
            "\n",
            "Samples: 40600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9525676965713501\n",
            "Loss G: 1.739502191543579\n",
            "\n",
            "Samples: 40700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7879390716552734\n",
            "Loss G: 1.9165230989456177\n",
            "\n",
            "Samples: 40800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9458612203598022\n",
            "Loss G: 1.8149008750915527\n",
            "\n",
            "Samples: 40900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0356807708740234\n",
            "Loss G: 1.5932636260986328\n",
            "\n",
            "Samples: 41000/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.2554298639297485\n",
            "Loss G: 1.329398512840271\n",
            "\n",
            "Samples: 41100/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.2975326776504517\n",
            "Loss G: 1.1745771169662476\n",
            "\n",
            "Samples: 41200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8419623374938965\n",
            "Loss G: 1.5430753231048584\n",
            "\n",
            "Samples: 41300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9033788442611694\n",
            "Loss G: 1.5569133758544922\n",
            "\n",
            "Samples: 41400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7978662848472595\n",
            "Loss G: 2.0289306640625\n",
            "\n",
            "Samples: 41500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7164158225059509\n",
            "Loss G: 2.334338665008545\n",
            "\n",
            "Samples: 41600/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0821598768234253\n",
            "Loss G: 2.0389292240142822\n",
            "\n",
            "Samples: 41700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7723410725593567\n",
            "Loss G: 1.7948578596115112\n",
            "\n",
            "Samples: 41800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9716674089431763\n",
            "Loss G: 1.3431938886642456\n",
            "\n",
            "Samples: 41900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6608126163482666\n",
            "Loss G: 1.3239846229553223\n",
            "\n",
            "Samples: 42000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8519665002822876\n",
            "Loss G: 1.2581723928451538\n",
            "\n",
            "Samples: 42100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8663874268531799\n",
            "Loss G: 1.5983622074127197\n",
            "\n",
            "Samples: 42200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8666526675224304\n",
            "Loss G: 1.7649775743484497\n",
            "\n",
            "Samples: 42300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7900755405426025\n",
            "Loss G: 2.101177215576172\n",
            "\n",
            "Samples: 42400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8040251731872559\n",
            "Loss G: 1.8544390201568604\n",
            "\n",
            "Samples: 42500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8295990228652954\n",
            "Loss G: 1.9731523990631104\n",
            "\n",
            "Samples: 42600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.80946946144104\n",
            "Loss G: 1.4781473875045776\n",
            "\n",
            "Samples: 42700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8563408851623535\n",
            "Loss G: 1.5395617485046387\n",
            "\n",
            "Samples: 42800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7614586353302002\n",
            "Loss G: 1.3827412128448486\n",
            "\n",
            "Samples: 42900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7111388444900513\n",
            "Loss G: 1.504297137260437\n",
            "\n",
            "Samples: 43000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6075135469436646\n",
            "Loss G: 1.8277126550674438\n",
            "\n",
            "Samples: 43100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.66205894947052\n",
            "Loss G: 1.9796847105026245\n",
            "\n",
            "Samples: 43200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7680187225341797\n",
            "Loss G: 1.8645620346069336\n",
            "\n",
            "Samples: 43300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5487223863601685\n",
            "Loss G: 1.8716561794281006\n",
            "\n",
            "Samples: 43400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5935715436935425\n",
            "Loss G: 1.8158493041992188\n",
            "\n",
            "Samples: 43500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7807952165603638\n",
            "Loss G: 1.7965121269226074\n",
            "\n",
            "Samples: 43600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7207455635070801\n",
            "Loss G: 1.8188223838806152\n",
            "\n",
            "Samples: 43700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8764919638633728\n",
            "Loss G: 1.884439468383789\n",
            "\n",
            "Samples: 43800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9772524833679199\n",
            "Loss G: 1.6985909938812256\n",
            "\n",
            "Samples: 43900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7612541913986206\n",
            "Loss G: 1.3463170528411865\n",
            "\n",
            "Samples: 44000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.663864254951477\n",
            "Loss G: 1.6024818420410156\n",
            "\n",
            "Samples: 44100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7258159518241882\n",
            "Loss G: 1.6669877767562866\n",
            "\n",
            "Samples: 44200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.550682783126831\n",
            "Loss G: 2.2042665481567383\n",
            "\n",
            "Samples: 44300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8526551723480225\n",
            "Loss G: 2.0696070194244385\n",
            "\n",
            "Samples: 44400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7760813236236572\n",
            "Loss G: 1.8745030164718628\n",
            "\n",
            "Samples: 44500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9193130731582642\n",
            "Loss G: 1.5392742156982422\n",
            "\n",
            "Samples: 44600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7864984273910522\n",
            "Loss G: 1.2317460775375366\n",
            "\n",
            "Samples: 44700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9498057961463928\n",
            "Loss G: 1.2122169733047485\n",
            "\n",
            "Samples: 44800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0334012508392334\n",
            "Loss G: 1.383687973022461\n",
            "\n",
            "Samples: 44900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.829066812992096\n",
            "Loss G: 1.6461917161941528\n",
            "\n",
            "Samples: 45000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9231679439544678\n",
            "Loss G: 1.9882372617721558\n",
            "\n",
            "Samples: 45100/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.046936273574829\n",
            "Loss G: 1.754590392112732\n",
            "\n",
            "Samples: 45200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8251140117645264\n",
            "Loss G: 1.6282299757003784\n",
            "\n",
            "Samples: 45300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7673185467720032\n",
            "Loss G: 1.3371609449386597\n",
            "\n",
            "Samples: 45400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7664185762405396\n",
            "Loss G: 1.479452133178711\n",
            "\n",
            "Samples: 45500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6556797623634338\n",
            "Loss G: 1.7544710636138916\n",
            "\n",
            "Samples: 45600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6022018194198608\n",
            "Loss G: 2.0862512588500977\n",
            "\n",
            "Samples: 45700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6804182529449463\n",
            "Loss G: 2.0251784324645996\n",
            "\n",
            "Samples: 45800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7049177885055542\n",
            "Loss G: 2.2090437412261963\n",
            "\n",
            "Samples: 45900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6816691160202026\n",
            "Loss G: 1.95387864112854\n",
            "\n",
            "Samples: 46000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6173454523086548\n",
            "Loss G: 1.8769643306732178\n",
            "\n",
            "Samples: 46100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6444752216339111\n",
            "Loss G: 1.7616366147994995\n",
            "\n",
            "Samples: 46200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.59970623254776\n",
            "Loss G: 1.7103674411773682\n",
            "\n",
            "Samples: 46300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5701401233673096\n",
            "Loss G: 1.8799550533294678\n",
            "\n",
            "Samples: 46400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.49268415570259094\n",
            "Loss G: 2.2606823444366455\n",
            "\n",
            "Samples: 46500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4662163257598877\n",
            "Loss G: 2.619678020477295\n",
            "\n",
            "Samples: 46600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4879126250743866\n",
            "Loss G: 2.9601714611053467\n",
            "\n",
            "Samples: 46700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6808421611785889\n",
            "Loss G: 2.655043840408325\n",
            "\n",
            "Samples: 46800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.613828182220459\n",
            "Loss G: 2.2817189693450928\n",
            "\n",
            "Samples: 46900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5760910511016846\n",
            "Loss G: 2.0283288955688477\n",
            "\n",
            "Samples: 47000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.4591185450553894\n",
            "Loss G: 1.7886227369308472\n",
            "\n",
            "Samples: 47100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5442425012588501\n",
            "Loss G: 1.7983219623565674\n",
            "\n",
            "Samples: 47200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5774742364883423\n",
            "Loss G: 1.7326611280441284\n",
            "\n",
            "Samples: 47300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5547254085540771\n",
            "Loss G: 1.9042109251022339\n",
            "\n",
            "Samples: 47400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6641745567321777\n",
            "Loss G: 1.9564132690429688\n",
            "\n",
            "Samples: 47500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7903428077697754\n",
            "Loss G: 1.9528310298919678\n",
            "\n",
            "Samples: 47600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6429202556610107\n",
            "Loss G: 2.0668582916259766\n",
            "\n",
            "Samples: 47700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6301205158233643\n",
            "Loss G: 2.109992265701294\n",
            "\n",
            "Samples: 47800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6794030666351318\n",
            "Loss G: 1.8204314708709717\n",
            "\n",
            "Samples: 47900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5056290626525879\n",
            "Loss G: 2.0131924152374268\n",
            "\n",
            "Samples: 48000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5264307260513306\n",
            "Loss G: 2.0771329402923584\n",
            "\n",
            "Samples: 48100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7850949764251709\n",
            "Loss G: 1.9935616254806519\n",
            "\n",
            "Samples: 48200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7412639856338501\n",
            "Loss G: 1.717077612876892\n",
            "\n",
            "Samples: 48300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8305925130844116\n",
            "Loss G: 1.5227347612380981\n",
            "\n",
            "Samples: 48400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9770572185516357\n",
            "Loss G: 1.5365608930587769\n",
            "\n",
            "Samples: 48500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8314714431762695\n",
            "Loss G: 1.7206666469573975\n",
            "\n",
            "Samples: 48600/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0797806978225708\n",
            "Loss G: 1.2729955911636353\n",
            "\n",
            "Samples: 48700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9867385625839233\n",
            "Loss G: 1.4855931997299194\n",
            "\n",
            "Samples: 48800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.388981580734253\n",
            "Loss G: 1.2576282024383545\n",
            "\n",
            "Samples: 48900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0169367790222168\n",
            "Loss G: 1.2923897504806519\n",
            "\n",
            "Samples: 49000/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.3369286060333252\n",
            "Loss G: 1.100919485092163\n",
            "\n",
            "Samples: 49100/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.3422584533691406\n",
            "Loss G: 1.0450514554977417\n",
            "\n",
            "Samples: 49200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.236670732498169\n",
            "Loss G: 1.1223430633544922\n",
            "\n",
            "Samples: 49300/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1073633432388306\n",
            "Loss G: 1.212058663368225\n",
            "\n",
            "Samples: 49400/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0723326206207275\n",
            "Loss G: 1.3407800197601318\n",
            "\n",
            "Samples: 49500/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0857287645339966\n",
            "Loss G: 1.6450814008712769\n",
            "\n",
            "Samples: 49600/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0942559242248535\n",
            "Loss G: 1.4442881345748901\n",
            "\n",
            "Samples: 49700/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.115277886390686\n",
            "Loss G: 1.3546301126480103\n",
            "\n",
            "Samples: 49800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0806121826171875\n",
            "Loss G: 1.2465968132019043\n",
            "\n",
            "Samples: 49900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9537638425827026\n",
            "Loss G: 1.2618999481201172\n",
            "\n",
            "Samples: 50000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9765021204948425\n",
            "Loss G: 1.488916277885437\n",
            "\n",
            "Samples: 50100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8568329811096191\n",
            "Loss G: 1.6185345649719238\n",
            "\n",
            "Samples: 50200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0071159601211548\n",
            "Loss G: 1.7174612283706665\n",
            "\n",
            "Samples: 50300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9979918003082275\n",
            "Loss G: 1.661957025527954\n",
            "\n",
            "Samples: 50400/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0225718021392822\n",
            "Loss G: 1.3242813348770142\n",
            "\n",
            "Samples: 50500/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.3683905601501465\n",
            "Loss G: 0.9217613935470581\n",
            "\n",
            "Samples: 50600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8230718970298767\n",
            "Loss G: 1.0270049571990967\n",
            "\n",
            "Samples: 50700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9339333176612854\n",
            "Loss G: 1.2364749908447266\n",
            "\n",
            "Samples: 50800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0616949796676636\n",
            "Loss G: 1.4775500297546387\n",
            "\n",
            "Samples: 50900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0088005065917969\n",
            "Loss G: 2.0502073764801025\n",
            "\n",
            "Samples: 51000/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.197638988494873\n",
            "Loss G: 2.193485736846924\n",
            "\n",
            "Samples: 51100/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.2424116134643555\n",
            "Loss G: 1.432008981704712\n",
            "\n",
            "Samples: 51200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9805749654769897\n",
            "Loss G: 1.1663095951080322\n",
            "\n",
            "Samples: 51300/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.2714052200317383\n",
            "Loss G: 0.7619054317474365\n",
            "\n",
            "Samples: 51400/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1798763275146484\n",
            "Loss G: 0.7902355194091797\n",
            "\n",
            "Samples: 51500/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1615036725997925\n",
            "Loss G: 1.0764832496643066\n",
            "\n",
            "Samples: 51600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6381796598434448\n",
            "Loss G: 1.872243881225586\n",
            "\n",
            "Samples: 51700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7344846129417419\n",
            "Loss G: 2.315647602081299\n",
            "\n",
            "Samples: 51800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0941557884216309\n",
            "Loss G: 2.698169469833374\n",
            "\n",
            "Samples: 51900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7603589296340942\n",
            "Loss G: 2.560506820678711\n",
            "\n",
            "Samples: 52000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9480006098747253\n",
            "Loss G: 2.025035858154297\n",
            "\n",
            "Samples: 52100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7130919694900513\n",
            "Loss G: 1.7037100791931152\n",
            "\n",
            "Samples: 52200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7925629615783691\n",
            "Loss G: 1.3128770589828491\n",
            "\n",
            "Samples: 52300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7125520706176758\n",
            "Loss G: 1.3592536449432373\n",
            "\n",
            "Samples: 52400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7425335645675659\n",
            "Loss G: 1.2924681901931763\n",
            "\n",
            "Samples: 52500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8130072355270386\n",
            "Loss G: 1.4950709342956543\n",
            "\n",
            "Samples: 52600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6294596195220947\n",
            "Loss G: 1.8050439357757568\n",
            "\n",
            "Samples: 52700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.42090731859207153\n",
            "Loss G: 2.44960880279541\n",
            "\n",
            "Samples: 52800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6157191395759583\n",
            "Loss G: 2.521531105041504\n",
            "\n",
            "Samples: 52900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8349909782409668\n",
            "Loss G: 2.5757415294647217\n",
            "\n",
            "Samples: 53000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8445578813552856\n",
            "Loss G: 2.212035894393921\n",
            "\n",
            "Samples: 53100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7949203252792358\n",
            "Loss G: 1.8350319862365723\n",
            "\n",
            "Samples: 53200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7965403199195862\n",
            "Loss G: 1.3329743146896362\n",
            "\n",
            "Samples: 53300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6692398190498352\n",
            "Loss G: 1.3311294317245483\n",
            "\n",
            "Samples: 53400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8893710374832153\n",
            "Loss G: 1.1814746856689453\n",
            "\n",
            "Samples: 53500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7280111312866211\n",
            "Loss G: 1.6940282583236694\n",
            "\n",
            "Samples: 53600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6606276035308838\n",
            "Loss G: 1.9865385293960571\n",
            "\n",
            "Samples: 53700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6466306447982788\n",
            "Loss G: 2.192521333694458\n",
            "\n",
            "Samples: 53800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7210626602172852\n",
            "Loss G: 2.483858823776245\n",
            "\n",
            "Samples: 53900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8116132020950317\n",
            "Loss G: 2.4796948432922363\n",
            "\n",
            "Samples: 54000/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1146326065063477\n",
            "Loss G: 1.9302407503128052\n",
            "\n",
            "Samples: 54100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9299350380897522\n",
            "Loss G: 1.7864919900894165\n",
            "\n",
            "Samples: 54200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7751959562301636\n",
            "Loss G: 1.4873336553573608\n",
            "\n",
            "Samples: 54300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6700950860977173\n",
            "Loss G: 1.4440168142318726\n",
            "\n",
            "Samples: 54400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9767870306968689\n",
            "Loss G: 1.3703296184539795\n",
            "\n",
            "Samples: 54500/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0145411491394043\n",
            "Loss G: 1.3183892965316772\n",
            "\n",
            "Samples: 54600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9586137533187866\n",
            "Loss G: 1.4632983207702637\n",
            "\n",
            "Samples: 54700/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0849661827087402\n",
            "Loss G: 1.4546960592269897\n",
            "\n",
            "Samples: 54800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0060354471206665\n",
            "Loss G: 1.5977964401245117\n",
            "\n",
            "Samples: 54900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7255367040634155\n",
            "Loss G: 1.7327022552490234\n",
            "\n",
            "Samples: 55000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7812227606773376\n",
            "Loss G: 2.0466058254241943\n",
            "\n",
            "Samples: 55100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7682290077209473\n",
            "Loss G: 2.0797767639160156\n",
            "\n",
            "Samples: 55200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.724639892578125\n",
            "Loss G: 2.257946014404297\n",
            "\n",
            "Samples: 55300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8173703551292419\n",
            "Loss G: 1.878286361694336\n",
            "\n",
            "Samples: 55400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8550384044647217\n",
            "Loss G: 1.7435888051986694\n",
            "\n",
            "Samples: 55500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7081414461135864\n",
            "Loss G: 2.0531723499298096\n",
            "\n",
            "Samples: 55600/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0178618431091309\n",
            "Loss G: 1.7957427501678467\n",
            "\n",
            "Samples: 55700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7032096982002258\n",
            "Loss G: 2.144286870956421\n",
            "\n",
            "Samples: 55800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.793982744216919\n",
            "Loss G: 1.7813076972961426\n",
            "\n",
            "Samples: 55900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5011081695556641\n",
            "Loss G: 2.0091826915740967\n",
            "\n",
            "Samples: 56000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6364494562149048\n",
            "Loss G: 1.5826795101165771\n",
            "\n",
            "Samples: 56100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6767649054527283\n",
            "Loss G: 1.9062285423278809\n",
            "\n",
            "Samples: 56200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5172969698905945\n",
            "Loss G: 2.465489387512207\n",
            "\n",
            "Samples: 56300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8372654318809509\n",
            "Loss G: 2.5910091400146484\n",
            "\n",
            "Samples: 56400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.784395694732666\n",
            "Loss G: 2.15041184425354\n",
            "\n",
            "Samples: 56500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7628864049911499\n",
            "Loss G: 1.8026061058044434\n",
            "\n",
            "Samples: 56600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9170742034912109\n",
            "Loss G: 1.8599249124526978\n",
            "\n",
            "Samples: 56700/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0229946374893188\n",
            "Loss G: 1.8244073390960693\n",
            "\n",
            "Samples: 56800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9722575545310974\n",
            "Loss G: 1.5129399299621582\n",
            "\n",
            "Samples: 56900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8689297437667847\n",
            "Loss G: 1.3830819129943848\n",
            "\n",
            "Samples: 57000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8312555551528931\n",
            "Loss G: 1.5456472635269165\n",
            "\n",
            "Samples: 57100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7786586880683899\n",
            "Loss G: 1.5569987297058105\n",
            "\n",
            "Samples: 57200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7090013027191162\n",
            "Loss G: 1.5254268646240234\n",
            "\n",
            "Samples: 57300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6873773336410522\n",
            "Loss G: 1.8060137033462524\n",
            "\n",
            "Samples: 57400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8860865831375122\n",
            "Loss G: 1.7469550371170044\n",
            "\n",
            "Samples: 57500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.855640172958374\n",
            "Loss G: 1.9674190282821655\n",
            "\n",
            "Samples: 57600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.722644031047821\n",
            "Loss G: 1.8235719203948975\n",
            "\n",
            "Samples: 57700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8339853882789612\n",
            "Loss G: 1.9020116329193115\n",
            "\n",
            "Samples: 57800/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1887266635894775\n",
            "Loss G: 1.352848768234253\n",
            "\n",
            "Samples: 57900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.0910619497299194\n",
            "Loss G: 1.2254629135131836\n",
            "\n",
            "Samples: 58000/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.425076961517334\n",
            "Loss G: 1.1546496152877808\n",
            "\n",
            "Samples: 58100/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9832404851913452\n",
            "Loss G: 1.1331533193588257\n",
            "\n",
            "Samples: 58200/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1621510982513428\n",
            "Loss G: 1.0716718435287476\n",
            "\n",
            "Samples: 58300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7183811068534851\n",
            "Loss G: 1.5583841800689697\n",
            "\n",
            "Samples: 58400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.5124979019165039\n",
            "Loss G: 2.0241408348083496\n",
            "\n",
            "Samples: 58500/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.9638137221336365\n",
            "Loss G: 2.1759042739868164\n",
            "\n",
            "Samples: 58600/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1586291790008545\n",
            "Loss G: 2.055711507797241\n",
            "\n",
            "Samples: 58700/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.6951754093170166\n",
            "Loss G: 1.6099915504455566\n",
            "\n",
            "Samples: 58800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8255594372749329\n",
            "Loss G: 1.1919190883636475\n",
            "\n",
            "Samples: 58900/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.2746831178665161\n",
            "Loss G: 0.8610467314720154\n",
            "\n",
            "Samples: 59000/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.4063317775726318\n",
            "Loss G: 0.6412923336029053\n",
            "\n",
            "Samples: 59100/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.1127707958221436\n",
            "Loss G: 0.9756516814231873\n",
            "\n",
            "Samples: 59200/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8084862232208252\n",
            "Loss G: 1.5485650300979614\n",
            "\n",
            "Samples: 59300/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8349867463111877\n",
            "Loss G: 1.9507147073745728\n",
            "\n",
            "Samples: 59400/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8433511257171631\n",
            "Loss G: 2.26568865776062\n",
            "\n",
            "Samples: 59500/60000\n",
            "Epoch: 6\n",
            "Loss D: 1.6437511444091797\n",
            "Loss G: 2.148709774017334\n",
            "\n",
            "Samples: 59600/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8632537126541138\n",
            "Loss G: 1.8919976949691772\n",
            "\n",
            "Samples: 59700/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.7698462605476379\n",
            "Loss G: 1.5315136909484863\n",
            "\n",
            "Samples: 59800/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8105082511901855\n",
            "Loss G: 1.6438791751861572\n",
            "\n",
            "Samples: 59900/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.8767549991607666\n",
            "Loss G: 1.240125060081482\n",
            "\n",
            "Samples: 60000/60000\n",
            "Epoch: 6\n",
            "Loss D: 0.6892322301864624\n",
            "Loss G: 1.2269997596740723\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKxVYQoytxym"
      },
      "source": [
        "# plot images\n",
        "noise = torch.randn(BATCHSIZE, BATCHSIZE, 1, 1, device=device)\n",
        "images = G(noise).to(device)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "ax1.imshow(images[64].detach().cpu()[0], cmap='gray')\n",
        "ax2.imshow(images[32].detach().cpu()[0], cmap='gray')"
      ],
      "execution_count": 147,
      "outputs": []
    }
  ]
}